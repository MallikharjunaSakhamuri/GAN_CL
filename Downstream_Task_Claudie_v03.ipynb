{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84121ae0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Malli\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Malli\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Malli\\anaconda3\\Lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Successfully loaded and adapted pre-trained model\n",
      "\n",
      "=== BACE Classification Task ===\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tupleBatch' object has no attribute 'stores_as'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 618\u001b[0m\n\u001b[0;32m    615\u001b[0m     \u001b[38;5;66;03m# Additional fairness metrics can be calculated as needed\u001b[39;00m\n\u001b[0;32m    617\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 618\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[1], line 461\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_data \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[0;32m    460\u001b[0m     batch_graphs, batch_labels \u001b[38;5;241m=\u001b[39m batch_data\n\u001b[1;32m--> 461\u001b[0m     batch \u001b[38;5;241m=\u001b[39m Batch\u001b[38;5;241m.\u001b[39mfrom_data_list(batch_graphs)\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Use Batch.from_data_list\u001b[39;00m\n\u001b[0;32m    462\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m encoder_model(batch\u001b[38;5;241m.\u001b[39mx, batch\u001b[38;5;241m.\u001b[39medge_index, batch\u001b[38;5;241m.\u001b[39medge_attr, batch\u001b[38;5;241m.\u001b[39mbatch)\n\u001b[0;32m    463\u001b[0m     test_embeddings\u001b[38;5;241m.\u001b[39mappend(embedding\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch_geometric\\data\\batch.py:93\u001b[0m, in \u001b[0;36mBatch.from_data_list\u001b[1;34m(cls, data_list, follow_batch, exclude_keys)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_data_list\u001b[39m(\u001b[38;5;28mcls\u001b[39m, data_list: List[BaseData],\n\u001b[0;32m     83\u001b[0m                    follow_batch: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     84\u001b[0m                    exclude_keys: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     85\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Constructs a :class:`~torch_geometric.data.Batch` object from a\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;124;03m    Python list of :class:`~torch_geometric.data.Data` or\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;124;03m    :class:`~torch_geometric.data.HeteroData` objects.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;124;03m    :obj:`follow_batch`.\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;124;03m    Will exclude any keys given in :obj:`exclude_keys`.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 93\u001b[0m     batch, slice_dict, inc_dict \u001b[38;5;241m=\u001b[39m collate(\n\u001b[0;32m     94\u001b[0m         \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m     95\u001b[0m         data_list\u001b[38;5;241m=\u001b[39mdata_list,\n\u001b[0;32m     96\u001b[0m         increment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     97\u001b[0m         add_batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_list[\u001b[38;5;241m0\u001b[39m], Batch),\n\u001b[0;32m     98\u001b[0m         follow_batch\u001b[38;5;241m=\u001b[39mfollow_batch,\n\u001b[0;32m     99\u001b[0m         exclude_keys\u001b[38;5;241m=\u001b[39mexclude_keys,\n\u001b[0;32m    100\u001b[0m     )\n\u001b[0;32m    102\u001b[0m     batch\u001b[38;5;241m.\u001b[39m_num_graphs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(data_list)\n\u001b[0;32m    103\u001b[0m     batch\u001b[38;5;241m.\u001b[39m_slice_dict \u001b[38;5;241m=\u001b[39m slice_dict\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch_geometric\\data\\collate.py:45\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(cls, data_list, increment, add_batch, follow_batch, exclude_keys)\u001b[0m\n\u001b[0;32m     42\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m()\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Create empty stores:\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m out\u001b[38;5;241m.\u001b[39mstores_as(data_list[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     47\u001b[0m follow_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(follow_batch \u001b[38;5;129;01mor\u001b[39;00m [])\n\u001b[0;32m     48\u001b[0m exclude_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(exclude_keys \u001b[38;5;129;01mor\u001b[39;00m [])\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tupleBatch' object has no attribute 'stores_as'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from torch_geometric.data import Data, DataLoader, Batch  # Import Batch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "import os\n",
    "import deepchem as dc\n",
    "from scipy import stats\n",
    "import dalex as dx\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "class MolecularGraphDataset:\n",
    "    def __init__(self, smiles_list, labels):\n",
    "        self.smiles_list = smiles_list\n",
    "        self.labels = labels\n",
    "        self.graphs = []\n",
    "        self.processed_labels = []\n",
    "        self._process_smiles()\n",
    "        \n",
    "    def _process_smiles(self):\n",
    "        for i, smiles in enumerate(self.smiles_list):\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is None:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                Chem.Kekulize(mol, clearAromaticFlags=True)\n",
    "                \n",
    "                # Enhanced atom features\n",
    "                atom_features_list = []\n",
    "                for atom in mol.GetAtoms():\n",
    "                    features = [\n",
    "                        atom.GetAtomicNum(),\n",
    "                        atom.GetDegree(),\n",
    "                        atom.GetFormalCharge(),\n",
    "                        int(atom.GetHybridization()),\n",
    "                        int(atom.GetIsAromatic()),\n",
    "                        int(atom.IsInRing()),\n",
    "                        atom.GetNumRadicalElectrons(),\n",
    "                        atom.GetTotalNumHs()\n",
    "                    ]\n",
    "                    atom_features_list.append(features)\n",
    "                    \n",
    "                x = torch.tensor(atom_features_list, dtype=torch.float)\n",
    "                \n",
    "                # Enhanced bond features\n",
    "                edge_indices = []\n",
    "                edge_features = []\n",
    "                for bond in mol.GetBonds():\n",
    "                    i = bond.GetBeginAtomIdx()\n",
    "                    j = bond.GetEndAtomIdx()\n",
    "                    \n",
    "                    features = [\n",
    "                        int(bond.GetBondType()),\n",
    "                        int(bond.GetIsConjugated()),\n",
    "                        int(bond.IsInRing()),\n",
    "                        int(bond.GetStereo()),\n",
    "                        bond.GetValenceContrib(bond.GetBeginAtom()),\n",
    "                        bond.GetValenceContrib(bond.GetEndAtom())\n",
    "                    ]\n",
    "                    \n",
    "                    edge_indices.extend([[i, j], [j, i]])\n",
    "                    edge_features.extend([features, features])\n",
    "                \n",
    "                if len(edge_indices) == 0:\n",
    "                    continue\n",
    "                \n",
    "                edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n",
    "                edge_attr = torch.tensor(edge_features, dtype=torch.float)\n",
    "                \n",
    "                data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "                self.graphs.append(data)\n",
    "                self.processed_labels.append(int(self.labels[i]))\n",
    "                \n",
    "            except Exception as e:\n",
    "                continue\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.graphs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.graphs[idx], self.processed_labels[idx]\n",
    "\n",
    "class EnhancedContrastiveEncoder(nn.Module):\n",
    "    def __init__(self, node_dim, edge_dim, hidden_dim=128, output_dim=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Enhanced node encoder with batch normalization\n",
    "        self.node_encoder = nn.Sequential(\n",
    "            nn.Linear(node_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Enhanced edge encoder\n",
    "        self.edge_encoder = nn.Sequential(\n",
    "            nn.Linear(edge_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Multiple graph convolution layers\n",
    "        self.graph_conv1 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.graph_conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.graph_conv3 = GCNConv(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "        \n",
    "        # Output projection\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.graph_attention = nn.MultiheadAttention(hidden_dim, num_heads=4)\n",
    "        \n",
    "        # Add regularization\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    \n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        # Node and edge encoding\n",
    "        x = self.node_encoder(x)\n",
    "        edge_attr = self.edge_encoder(edge_attr)\n",
    "        \n",
    "        # Graph convolutions with residual connections\n",
    "        x1 = F.relu(self.graph_conv1(x, edge_index))\n",
    "        x2 = F.relu(self.graph_conv2(x1, edge_index)) + x1\n",
    "        x3 = self.graph_conv3(x2, edge_index) + x2\n",
    "        \n",
    "        # Apply layer normalization\n",
    "        x3 = self.layer_norm(x3)\n",
    "        \n",
    "        # Attention-weighted pooling\n",
    "        attention_weights = self.attention(x3)\n",
    "        attention_weights = F.softmax(attention_weights, dim=0)\n",
    "        attended_features = torch.sum(x3 * attention_weights, dim=0)\n",
    "        \n",
    "        # Global mean pooling\n",
    "        pooled_features = global_mean_pool(x3, batch)\n",
    "        \n",
    "        # Concatenate attended and pooled features\n",
    "        combined_features = torch.cat([attended_features.unsqueeze(0).expand(pooled_features.size(0), -1), \n",
    "                                       pooled_features], dim=1)\n",
    "        \n",
    "        # Final output\n",
    "        out = self.output(combined_features)\n",
    "        return F.normalize(out, dim=1)\n",
    "\n",
    "class MolecularClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims=(256, 128)):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        last_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(last_dim, hidden_dim))\n",
    "            layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(0.2))\n",
    "            last_dim = hidden_dim\n",
    "        layers.append(nn.Linear(last_dim, 1))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "def compute_fairness_loss(predictions, labels, sensitive_attributes):\n",
    "    \"\"\"\n",
    "    Compute fairness loss based on demographic parity and equal opportunity.\n",
    "    \"\"\"\n",
    "    epsilon = 1e-6\n",
    "    privileged_mask = (sensitive_attributes == 1)\n",
    "    unprivileged_mask = (sensitive_attributes == 0)\n",
    "    \n",
    "    # Demographic Parity Difference\n",
    "    mean_pred_privileged = predictions[privileged_mask].mean()\n",
    "    mean_pred_unprivileged = predictions[unprivileged_mask].mean()\n",
    "    dp_difference = torch.abs(mean_pred_privileged - mean_pred_unprivileged)\n",
    "    \n",
    "    # Equal Opportunity Difference (True Positive Rates)\n",
    "    true_labels_privileged = labels[privileged_mask]\n",
    "    true_labels_unprivileged = labels[unprivileged_mask]\n",
    "    \n",
    "    predictions_privileged = predictions[privileged_mask]\n",
    "    predictions_unprivileged = predictions[unprivileged_mask]\n",
    "    \n",
    "    tpr_privileged = ((predictions_privileged >= 0.5) & (true_labels_privileged == 1)).sum() / (true_labels_privileged == 1).sum().float().clamp(min=epsilon)\n",
    "    tpr_unprivileged = ((predictions_unprivileged >= 0.5) & (true_labels_unprivileged == 1)).sum() / (true_labels_unprivileged == 1).sum().float().clamp(min=epsilon)\n",
    "    eo_difference = torch.abs(tpr_privileged - tpr_unprivileged)\n",
    "    \n",
    "    fairness_loss = dp_difference + eo_difference\n",
    "    return fairness_loss\n",
    "\n",
    "def visualize_embedding_distribution(embeddings, labels, title=\"Embedding Distribution\"):\n",
    "    \"\"\"Visualize the distribution of embeddings using t-SNE.\"\"\"\n",
    "    from sklearn.manifold import TSNE\n",
    "    \n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    embeddings_2d = tsne.fit_transform(embeddings)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=labels, cmap='viridis')\n",
    "    plt.colorbar(scatter)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"t-SNE dimension 1\")\n",
    "    plt.ylabel(\"t-SNE dimension 2\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, title=\"Confusion Matrix\"):\n",
    "    \"\"\"Plot confusion matrix with enhanced visualization.\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "\n",
    "def plot_roc_curve(y_true, y_proba, title=\"ROC Curve\"):\n",
    "    \"\"\"Plot ROC curve with additional metrics.\"\"\"\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "def create_molecular_groups_validated(test_dataset):\n",
    "    \"\"\"Enhanced molecular feature groups with additional chemical criteria.\"\"\"\n",
    "    smiles_list = test_dataset.smiles_list\n",
    "    \n",
    "    properties = {\n",
    "        'MW': [], 'LogP': [], 'HBD': [], 'HBA': [], 'TPSA': [], \n",
    "        'RotBonds': [], 'AromaticRings': [], 'HeteroAtoms': []\n",
    "    }\n",
    "    \n",
    "    thresholds = {\n",
    "        'MW': 500, 'LogP': 5, 'HBD': 5, 'HBA': 10, \n",
    "        'TPSA': 90, 'RotBonds': 8\n",
    "    }\n",
    "    \n",
    "    for smiles in smiles_list:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is not None:\n",
    "            properties['MW'].append(Descriptors.ExactMolWt(mol))\n",
    "            properties['LogP'].append(Descriptors.MolLogP(mol))\n",
    "            properties['HBD'].append(Descriptors.NumHDonors(mol))\n",
    "            properties['HBA'].append(Descriptors.NumHAcceptors(mol))\n",
    "            properties['TPSA'].append(Descriptors.TPSA(mol))\n",
    "            properties['RotBonds'].append(Descriptors.NumRotatableBonds(mol))\n",
    "            properties['AromaticRings'].append(len(Chem.GetSymmSSSR(mol)))\n",
    "            properties['HeteroAtoms'].append(len([atom for atom in mol.GetAtoms() \n",
    "                                                if atom.GetAtomicNum() not in [6, 1]]))\n",
    "        else:\n",
    "            for key in properties:\n",
    "                properties[key].append(None)\n",
    "    \n",
    "    df = pd.DataFrame(properties)\n",
    "    \n",
    "    # Enhanced feature groups\n",
    "    feature_groups = {}\n",
    "    \n",
    "    # Ro5 compliance\n",
    "    ro5_violations = np.zeros(len(df))\n",
    "    for prop, threshold in thresholds.items():\n",
    "        if prop in ['MW', 'LogP', 'HBD', 'HBA']:\n",
    "            violations = (df[prop] > threshold).astype(int)\n",
    "            ro5_violations += violations\n",
    "    \n",
    "    feature_groups['Ro5_compliant'] = (ro5_violations < 2).astype(int)\n",
    "    \n",
    "    # BBB penetration\n",
    "    feature_groups['BBB_favorable'] = ((df['MW'] <= 400) & \n",
    "                                     (df['TPSA'] <= 90) & \n",
    "                                     (df['LogP'] <= 5) & \n",
    "                                     (df['HBD'] <= 3)).astype(int)\n",
    "    \n",
    "    # BACE binding\n",
    "    feature_groups['BACE_favorable'] = ((df['MW'] >= 300) & \n",
    "                                      (df['MW'] <= 600) & \n",
    "                                      (df['LogP'] >= 1) & \n",
    "                                      (df['LogP'] <= 5)).astype(int)\n",
    "    \n",
    "    # Add custom groups\n",
    "    for name, group in feature_groups.items():\n",
    "        df[name] = group\n",
    "    \n",
    "    return df\n",
    "\n",
    "def adapt_state_dict(checkpoint_state_dict, model):\n",
    "    \"\"\"Adapt the saved state dict to work with the enhanced model architecture.\"\"\"\n",
    "    new_state_dict = {}\n",
    "    model_state_dict = model.state_dict()\n",
    "    \n",
    "    # Initialize new state dict with current model's structure\n",
    "    for key in model_state_dict.keys():\n",
    "        if key in checkpoint_state_dict:\n",
    "            if model_state_dict[key].shape == checkpoint_state_dict[key].shape:\n",
    "                new_state_dict[key] = checkpoint_state_dict[key]\n",
    "            else:\n",
    "                # Handle dimension mismatches\n",
    "                if 'node_encoder.0.weight' in key:\n",
    "                    # Adapt node encoder weights\n",
    "                    old_weight = checkpoint_state_dict[key]\n",
    "                    new_weight = torch.zeros(model_state_dict[key].shape)\n",
    "                    min_dim = min(old_weight.shape[1], new_weight.shape[1])\n",
    "                    new_weight[:, :min_dim] = old_weight[:, :min_dim]\n",
    "                    new_state_dict[key] = new_weight\n",
    "                elif 'edge_encoder.0.weight' in key:\n",
    "                    # Adapt edge encoder weights\n",
    "                    old_weight = checkpoint_state_dict[key]\n",
    "                    new_weight = torch.zeros(model_state_dict[key].shape)\n",
    "                    min_dim = min(old_weight.shape[1], new_weight.shape[1])\n",
    "                    new_weight[:, :min_dim] = old_weight[:, :min_dim]\n",
    "                    new_state_dict[key] = new_weight\n",
    "                else:\n",
    "                    # Initialize other mismatched layers with current model's values\n",
    "                    new_state_dict[key] = model_state_dict[key]\n",
    "        else:\n",
    "            # Initialize new layers with current model's values\n",
    "            new_state_dict[key] = model_state_dict[key]\n",
    "    \n",
    "    return new_state_dict\n",
    "\n",
    "def main():\n",
    "    # Previous parameters remain the same\n",
    "    contrastive_model_path = 'molecular_gan_cl_models_CA11.pt'\n",
    "    input_dim = 8  # Enhanced atom features\n",
    "    edge_dim = 6   # Enhanced bond features\n",
    "    hidden_dim = 128\n",
    "    output_dim = 128\n",
    "    \n",
    "    # Initialize the enhanced encoder\n",
    "    encoder_model = EnhancedContrastiveEncoder(\n",
    "        node_dim=input_dim,\n",
    "        edge_dim=edge_dim,\n",
    "        hidden_dim=hidden_dim,\n",
    "        output_dim=output_dim\n",
    "    ).to(device)\n",
    "    \n",
    "    # Load and adapt the pre-trained weights\n",
    "    try:\n",
    "        checkpoint = torch.load(contrastive_model_path, map_location=device)\n",
    "        adapted_state_dict = adapt_state_dict(checkpoint['encoder_state_dict'], encoder_model)\n",
    "        encoder_model.load_state_dict(adapted_state_dict, strict=False)\n",
    "        print(\"Successfully loaded and adapted pre-trained model\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {str(e)}\")\n",
    "        return\n",
    "\n",
    "    # Modified initialization for batch normalization\n",
    "    encoder_model.train()\n",
    "    with torch.no_grad():\n",
    "        # Create proper batch size for initialization\n",
    "        batch_size = 32\n",
    "        num_nodes = 16  # Multiple nodes per graph\n",
    "        num_edges = 32  # Multiple edges per graph\n",
    "        \n",
    "        # Create dummy batch with multiple nodes per graph\n",
    "        dummy_data = torch.randn(batch_size * num_nodes, input_dim).to(device)\n",
    "        \n",
    "        # Create edge indices that connect nodes within each graph\n",
    "        edge_indices = []\n",
    "        edge_attrs = []\n",
    "        batch_assignments = []\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            # Create edges for current graph\n",
    "            start_idx = i * num_nodes\n",
    "            for j in range(num_edges):\n",
    "                src = start_idx + torch.randint(0, num_nodes, (1,)).item()\n",
    "                dst = start_idx + torch.randint(0, num_nodes, (1,)).item()\n",
    "                edge_indices.extend([[src, dst]])\n",
    "                edge_attrs.append(torch.randn(edge_dim))\n",
    "            \n",
    "            # Batch assignments for current graph\n",
    "            batch_assignments.extend([i] * num_nodes)\n",
    "        \n",
    "        dummy_edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous().to(device)\n",
    "        dummy_edge_attr = torch.stack(edge_attrs).to(device)\n",
    "        dummy_batch = torch.tensor(batch_assignments, dtype=torch.long).to(device)\n",
    "        \n",
    "        # Initialize BatchNorm statistics with proper batched data\n",
    "        for _ in range(10):\n",
    "            encoder_model(dummy_data, dummy_edge_index, dummy_edge_attr, dummy_batch)\n",
    "    \n",
    "    encoder_model.eval()\n",
    "    \n",
    "    print(\"\\n=== BACE Classification Task ===\")\n",
    "    \n",
    "    # Prepare data\n",
    "    tasks, datasets, transformers = dc.molnet.load_bace_classification(\n",
    "        featurizer='Raw',\n",
    "        splitter='scaffold',\n",
    "        transformers=[],\n",
    "        reload=True\n",
    "    )\n",
    "    train_dataset, valid_dataset, test_dataset = datasets\n",
    "    \n",
    "    # Combine train and validation datasets for cross-validation\n",
    "    combined_smiles = train_dataset.ids.tolist() + valid_dataset.ids.tolist()\n",
    "    combined_labels = np.concatenate([train_dataset.y[:, 0], valid_dataset.y[:, 0]])\n",
    "    \n",
    "    # Process datasets\n",
    "    combined_data = MolecularGraphDataset(\n",
    "        combined_smiles,\n",
    "        combined_labels.tolist()\n",
    "    )\n",
    "    \n",
    "    test_data = MolecularGraphDataset(\n",
    "        test_dataset.ids.tolist(),\n",
    "        test_dataset.y[:, 0].tolist()\n",
    "    )\n",
    "    \n",
    "    # Generate embeddings for test data\n",
    "    test_loader = DataLoader([(data, label) for data, label in zip(test_data.graphs, test_data.processed_labels)], batch_size=32, shuffle=False)\n",
    "    test_embeddings = []\n",
    "    y_test = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_data in test_loader:\n",
    "            batch_graphs, batch_labels = batch_data\n",
    "            batch = Batch.from_data_list(batch_graphs).to(device)  # Use Batch.from_data_list\n",
    "            embedding = encoder_model(batch.x, batch.edge_index, batch.edge_attr, batch.batch)\n",
    "            test_embeddings.append(embedding.cpu().numpy())\n",
    "            y_test.extend(batch_labels)\n",
    "    \n",
    "    X_test = np.vstack(test_embeddings)\n",
    "    y_test = np.array(y_test)\n",
    "    \n",
    "    # Stratified K-Fold Cross-Validation\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    fold_accuracies = []\n",
    "    fold_auc_scores = []\n",
    "    \n",
    "    y_combined = np.array(combined_data.processed_labels)\n",
    "    sensitive_attributes_df = create_molecular_groups_validated(combined_data)\n",
    "    sensitive_attributes = sensitive_attributes_df['Ro5_compliant'].values  # Use appropriate length\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(np.zeros(len(y_combined)), y_combined)):\n",
    "        print(f\"\\n=== Fold {fold + 1} ===\")\n",
    "        # Prepare train and validation data\n",
    "        train_graphs = [combined_data.graphs[i] for i in train_idx]\n",
    "        train_labels = [combined_data.processed_labels[i] for i in train_idx]\n",
    "        val_graphs = [combined_data.graphs[i] for i in val_idx]\n",
    "        val_labels = [combined_data.processed_labels[i] for i in val_idx]\n",
    "        \n",
    "        # Generate embeddings for training data\n",
    "        train_loader = DataLoader([(data, label) for data, label in zip(train_graphs, train_labels)], batch_size=32, shuffle=True)\n",
    "        val_loader = DataLoader([(data, label) for data, label in zip(val_graphs, val_labels)], batch_size=32, shuffle=False)\n",
    "        \n",
    "        # Initialize classifier\n",
    "        classifier = MolecularClassifier(input_dim=output_dim).to(device)\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "        \n",
    "        # Get sensitive attributes for the current training batch\n",
    "        train_sensitive_attributes = sensitive_attributes[train_idx]\n",
    "        val_sensitive_attributes = sensitive_attributes[val_idx]\n",
    "        \n",
    "        # Training loop\n",
    "        num_epochs = 20\n",
    "        for epoch in range(num_epochs):\n",
    "            classifier.train()\n",
    "            epoch_loss = 0\n",
    "            for batch_data in train_loader:\n",
    "                batch_graphs, batch_labels = batch_data\n",
    "                batch = Batch.from_data_list(batch_graphs).to(device)  # Use Batch.from_data_list\n",
    "                embeddings = encoder_model(batch.x, batch.edge_index, batch.edge_attr, batch.batch)\n",
    "                outputs = classifier(embeddings).squeeze()\n",
    "                labels = torch.tensor(batch_labels, dtype=torch.float32).to(device)\n",
    "                \n",
    "                # Fairness loss\n",
    "                batch_indices = [train_idx[i] for i in range(len(batch_labels))]  # Get original indices\n",
    "                batch_sensitive = torch.tensor(train_sensitive_attributes[batch_indices], dtype=torch.float32).to(device)\n",
    "                fairness_loss = compute_fairness_loss(torch.sigmoid(outputs), labels, batch_sensitive)\n",
    "                \n",
    "                # Total loss\n",
    "                loss = criterion(outputs, labels) + 0.1 * fairness_loss\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss / len(train_loader):.4f}\")\n",
    "        \n",
    "        # Evaluation on validation set\n",
    "        classifier.eval()\n",
    "        val_outputs = []\n",
    "        val_labels_list = []\n",
    "        with torch.no_grad():\n",
    "            for batch_data in val_loader:\n",
    "                batch_graphs, batch_labels = batch_data\n",
    "                batch = Batch.from_data_list(batch_graphs).to(device)  # Use Batch.from_data_list\n",
    "                embeddings = encoder_model(batch.x, batch.edge_index, batch.edge_attr, batch.batch)\n",
    "                outputs = classifier(embeddings).squeeze()\n",
    "                val_outputs.extend(torch.sigmoid(outputs).cpu().numpy())\n",
    "                val_labels_list.extend(batch_labels)\n",
    "        \n",
    "        val_preds = (np.array(val_outputs) >= 0.5).astype(int)\n",
    "        val_labels_array = np.array(val_labels_list)\n",
    "        accuracy = accuracy_score(val_labels_array, val_preds)\n",
    "        auc_score = roc_auc_score(val_labels_array, val_outputs)\n",
    "        fold_accuracies.append(accuracy)\n",
    "        fold_auc_scores.append(auc_score)\n",
    "        print(f\"Validation Accuracy: {accuracy:.4f}, AUC: {auc_score:.4f}\")\n",
    "    \n",
    "    print(\"\\n=== Cross-Validation Results ===\")\n",
    "    print(f\"Average Accuracy: {np.mean(fold_accuracies):.4f}\")\n",
    "    print(f\"Average AUC: {np.mean(fold_auc_scores):.4f}\")\n",
    "    \n",
    "    # Retrain on full training data\n",
    "    train_loader = DataLoader([(data, label) for data, label in zip(combined_data.graphs, combined_data.processed_labels)], batch_size=32, shuffle=True)\n",
    "    classifier = MolecularClassifier(input_dim=output_dim).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "    \n",
    "    # Training loop\n",
    "    num_epochs = 20\n",
    "    for epoch in range(num_epochs):\n",
    "        classifier.train()\n",
    "        epoch_loss = 0\n",
    "        for batch_data in train_loader:\n",
    "            batch_graphs, batch_labels = batch_data\n",
    "            batch = Batch.from_data_list(batch_graphs).to(device)  # Use Batch.from_data_list\n",
    "            embeddings = encoder_model(batch.x, batch.edge_index, batch.edge_attr, batch.batch)\n",
    "            outputs = classifier(embeddings).squeeze()\n",
    "            labels = torch.tensor(batch_labels, dtype=torch.float32).to(device)\n",
    "            \n",
    "            # Fairness loss\n",
    "            batch_indices = [i for i in range(len(batch_labels))]\n",
    "            batch_sensitive = torch.tensor(sensitive_attributes[batch_indices], dtype=torch.float32).to(device)\n",
    "            fairness_loss = compute_fairness_loss(torch.sigmoid(outputs), labels, batch_sensitive)\n",
    "            \n",
    "            # Total loss\n",
    "            loss = criterion(outputs, labels) + 0.1 * fairness_loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss / len(train_loader):.4f}\")\n",
    "    \n",
    "    # Evaluation on test set\n",
    "    classifier.eval()\n",
    "    test_outputs = []\n",
    "    with torch.no_grad():\n",
    "        for batch_data in test_loader:\n",
    "            batch_graphs, batch_labels = batch_data\n",
    "            batch = Batch.from_data_list(batch_graphs).to(device)  # Use Batch.from_data_list\n",
    "            embeddings = encoder_model(batch.x, batch.edge_index, batch.edge_attr, batch.batch)\n",
    "            outputs = classifier(embeddings).squeeze()\n",
    "            test_outputs.extend(torch.sigmoid(outputs).cpu().numpy())\n",
    "    \n",
    "    y_pred = (np.array(test_outputs) >= 0.5).astype(int)\n",
    "    y_pred_proba = np.array(test_outputs)\n",
    "    \n",
    "    print(\"\\n=== Test Set Evaluation ===\")\n",
    "    plot_confusion_matrix(y_test, y_pred, \"BACE Classification Confusion Matrix\")\n",
    "    print(\"\\n=== Classification Metrics ===\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Visualizations\n",
    "    plot_roc_curve(y_test, y_pred_proba, \"BACE Classification ROC Curve\")\n",
    "    visualize_embedding_distribution(X_test, y_test, \"BACE Embedding Distribution\")\n",
    "    \n",
    "    # Fairness analysis\n",
    "    print(\"\\n=== Fairness Analysis ===\")\n",
    "    # Since the classifier is now a PyTorch model, we need to adjust the fairness analysis\n",
    "    # We can use the test outputs and the sensitive attributes from the test set\n",
    "    test_sensitive_attributes_df = create_molecular_groups_validated(test_data)\n",
    "    test_sensitive_attributes = test_sensitive_attributes_df['Ro5_compliant'].values[:len(y_test)]\n",
    "    test_sensitive_attributes = torch.tensor(test_sensitive_attributes, dtype=torch.float32).to(device)\n",
    "    test_labels = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "    test_predictions = torch.tensor(y_pred_proba, dtype=torch.float32).to(device)\n",
    "    fairness_loss = compute_fairness_loss(test_predictions, test_labels, test_sensitive_attributes)\n",
    "    print(f\"Fairness Loss on Test Set: {fairness_loss.item():.4f}\")\n",
    "    \n",
    "    # Additional fairness metrics can be calculated as needed\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (MyEnv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
