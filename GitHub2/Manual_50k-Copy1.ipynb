{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5379f427",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Malli\\anaconda3\\envs\\baceenv\\lib\\site-packages\\torch_geometric\\typing.py:86: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: [WinError 127] The specified procedure could not be found\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, Descriptors\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "import random\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Data\n",
    "from rdkit import RDLogger\n",
    "from rdkit.Chem import RemoveHs\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.data import DataLoader\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool, MessagePassing\n",
    "from typing import Tuple, List, Optional\n",
    "import copy\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, Descriptors\n",
    "from rdkit.Chem.EnumerateStereoisomers import EnumerateStereoisomers, StereoEnumerationOptions\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import copy\n",
    "from torch_geometric.data import Data, Batch, DataLoader\n",
    "import os\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "# Suppress RDKit warnings\n",
    "RDLogger.DisableLog('rdApp.warning')\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8de17bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MolecularFeatureExtractor:\n",
    "    def __init__(self):\n",
    "        self.atom_list = list(range(1, 119))\n",
    "        self.chirality_list = [\n",
    "            Chem.rdchem.ChiralType.CHI_UNSPECIFIED,\n",
    "            Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CW,\n",
    "            Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CCW,\n",
    "            Chem.rdchem.ChiralType.CHI_OTHER\n",
    "        ]\n",
    "        self.bond_list = [\n",
    "            Chem.rdchem.BondType.SINGLE,\n",
    "            Chem.rdchem.BondType.DOUBLE, \n",
    "            Chem.rdchem.BondType.TRIPLE,\n",
    "            Chem.rdchem.BondType.AROMATIC\n",
    "        ]\n",
    "        self.bonddir_list = [\n",
    "            Chem.rdchem.BondDir.NONE,\n",
    "            Chem.rdchem.BondDir.ENDUPRIGHT,\n",
    "            Chem.rdchem.BondDir.ENDDOWNRIGHT\n",
    "        ]\n",
    "\n",
    "    def calc_atom_features(self, atom: Chem.Atom) -> Tuple[list, list]:\n",
    "        \"\"\"Calculate atom features with better error handling\"\"\"\n",
    "        try:\n",
    "            # Basic features\n",
    "            atom_feat = [\n",
    "                self.atom_list.index(atom.GetAtomicNum()),\n",
    "                self.chirality_list.index(atom.GetChiralTag())\n",
    "            ]\n",
    "\n",
    "            # Physical features with error handling\n",
    "            phys_feat = []\n",
    "            \n",
    "            # Molecular weight contribution\n",
    "            try:\n",
    "                contrib_mw = Descriptors.ExactMolWt(Chem.MolFromSmiles(f'[{atom.GetSymbol()}]'))\n",
    "                phys_feat.append(contrib_mw)\n",
    "            except:\n",
    "                phys_feat.append(0.0)\n",
    "                \n",
    "            # LogP contribution    \n",
    "            try:\n",
    "                contrib_logp = Descriptors.MolLogP(Chem.MolFromSmiles(f'[{atom.GetSymbol()}]'))\n",
    "                phys_feat.append(contrib_logp)\n",
    "            except:\n",
    "                phys_feat.append(0.0)\n",
    "                \n",
    "            # Add other physical properties\n",
    "            phys_feat.extend([\n",
    "                atom.GetFormalCharge(),\n",
    "                int(atom.GetHybridization()),\n",
    "                int(atom.GetIsAromatic()),\n",
    "                atom.GetTotalNumHs(),\n",
    "                atom.GetTotalValence(),\n",
    "                atom.GetDegree()\n",
    "            ])\n",
    "            \n",
    "            return atom_feat, phys_feat\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating atom features: {e}\")\n",
    "            return [0, 0], [0.0] * 9\n",
    "\n",
    "    def get_atom_features(self, mol: Chem.Mol) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Extract atom features for the whole molecule\"\"\"\n",
    "        atom_feats = []\n",
    "        phys_feats = []\n",
    "        \n",
    "        if mol is None:\n",
    "            return torch.tensor([[0, 0]], dtype=torch.long), torch.tensor([[0.0] * 9], dtype=torch.float)\n",
    "            \n",
    "        for atom in mol.GetAtoms():\n",
    "            atom_feat, phys_feat = self.calc_atom_features(atom)\n",
    "            atom_feats.append(atom_feat)\n",
    "            phys_feats.append(phys_feat)\n",
    "\n",
    "        x = torch.tensor(atom_feats, dtype=torch.long)\n",
    "        phys = torch.tensor(phys_feats, dtype=torch.float)\n",
    "        \n",
    "        return x, phys\n",
    "    \n",
    "    def remove_unbonded_hydrogens(mol):\n",
    "        params = Chem.RemoveHsParameters()\n",
    "        params.removeDegreeZero = True\n",
    "        mol = Chem.RemoveHs(mol, params)\n",
    "        return mol\n",
    "\n",
    "\n",
    "    def get_bond_features(self, mol: Chem.Mol) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Extract bond features with better error handling\"\"\"\n",
    "        if mol is None:\n",
    "            return torch.tensor([[0], [0]], dtype=torch.long), torch.tensor([[0.0] * 5], dtype=torch.float)\n",
    "            \n",
    "        row, col, edge_feat = [], [], []\n",
    "        \n",
    "        for bond in mol.GetBonds():\n",
    "            try:\n",
    "                start, end = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
    "                \n",
    "                # Add edges in both directions\n",
    "                row += [start, end]\n",
    "                col += [end, start]\n",
    "                \n",
    "                # Bond features\n",
    "                bond_type = self.bond_list.index(bond.GetBondType())\n",
    "                bond_dir = self.bonddir_list.index(bond.GetBondDir())\n",
    "                \n",
    "                # Calculate additional properties\n",
    "                feat = [\n",
    "                    bond_type,\n",
    "                    bond_dir,\n",
    "                    int(bond.GetIsConjugated()),\n",
    "                    int(self._is_rotatable(bond)),\n",
    "                    self._get_bond_length(mol, start, end)\n",
    "                ]\n",
    "                \n",
    "                edge_feat.extend([feat, feat])\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing bond: {e}\")\n",
    "                continue\n",
    "\n",
    "        if not row:  # If no valid bonds were processed\n",
    "            return torch.tensor([[0], [0]], dtype=torch.long), torch.tensor([[0.0] * 5], dtype=torch.float)\n",
    "\n",
    "        edge_index = torch.tensor([row, col], dtype=torch.long)\n",
    "        edge_attr = torch.tensor(edge_feat, dtype=torch.float)\n",
    "        \n",
    "        return edge_index, edge_attr\n",
    "\n",
    "    def _is_rotatable(self, bond: Chem.Bond) -> bool:\n",
    "        \"\"\"Check if bond is rotatable\"\"\"\n",
    "        return (bond.GetBondType() == Chem.rdchem.BondType.SINGLE and \n",
    "                not bond.IsInRing() and\n",
    "                len(bond.GetBeginAtom().GetNeighbors()) > 1 and\n",
    "                len(bond.GetEndAtom().GetNeighbors()) > 1)\n",
    "\n",
    "    def _get_bond_length(self, mol: Chem.Mol, start: int, end: int) -> float:\n",
    "        \"\"\"Get bond length with error handling\"\"\"\n",
    "        try:\n",
    "            conf = mol.GetConformer()\n",
    "            if conf.Is3D():\n",
    "                return Chem.rdMolTransforms.GetBondLength(conf, start, end)\n",
    "        except:\n",
    "            pass\n",
    "        return 0.0\n",
    "\n",
    "    def process_molecule(self, smiles: str) -> Data:\n",
    "        \"\"\"Process SMILES string to graph data\"\"\"\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is None:\n",
    "                print(f\"Invalid SMILES: {smiles}\")\n",
    "                return None  # Skip invalid molecules\n",
    "            mol = RemoveHs(mol)\n",
    "\n",
    "            # Add explicit hydrogens\n",
    "            mol = Chem.AddHs(mol, addCoords=True)\n",
    "\n",
    "            # Sanitize molecule\n",
    "            Chem.SanitizeMol(mol)\n",
    "\n",
    "            # Check if the molecule has atoms\n",
    "            if mol.GetNumAtoms() == 0:\n",
    "                print(\"Molecule has no atoms, skipping.\")\n",
    "                return None\n",
    "\n",
    "            # Generate 3D coordinates\n",
    "            if not mol.GetNumConformers():\n",
    "                status = AllChem.EmbedMolecule(mol, AllChem.ETKDG())\n",
    "                if status != 0:\n",
    "                    print(\"Failed to generate 3D conformer\")\n",
    "                    return None  # Skip failed molecules\n",
    "\n",
    "                # Try MMFF or UFF optimization\n",
    "                try:\n",
    "                    AllChem.MMFFOptimizeMolecule(mol)\n",
    "                except:\n",
    "                    AllChem.UFFOptimizeMolecule(mol)\n",
    "\n",
    "            # Extract features\n",
    "            x_cat, x_phys = self.get_atom_features(mol)\n",
    "            edge_index, edge_attr = self.get_bond_features(mol)\n",
    "\n",
    "            # Create data object with SMILES\n",
    "            data = Data(\n",
    "                x_cat=x_cat, \n",
    "                x_phys=x_phys,\n",
    "                edge_index=edge_index, \n",
    "                edge_attr=edge_attr,\n",
    "                num_nodes=x_cat.size(0)\n",
    "            )\n",
    "\n",
    "            # Store the SMILES as an attribute\n",
    "            data.smiles = smiles\n",
    "\n",
    "            return data\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing molecule {smiles}: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35a311ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, Descriptors\n",
    "from rdkit.Chem.EnumerateStereoisomers import EnumerateStereoisomers, StereoEnumerationOptions\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import copy\n",
    "from torch_geometric.data import Data, Batch, DataLoader\n",
    "import os\n",
    "import pickle\n",
    "from rdkit.Chem import MolStandardize\n",
    "from collections import defaultdict\n",
    "\n",
    "class MolecularAugmenter:\n",
    "    \"\"\"Class for applying chemical augmentations to molecules\"\"\"\n",
    "    \n",
    "    def __init__(self, smiles_list, feature_extractor=None, seed=42):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            smiles_list: List of SMILES strings to augment\n",
    "            feature_extractor: The feature extractor used to convert SMILES to graph data\n",
    "            seed: Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        self.smiles_list = smiles_list\n",
    "        self.feature_extractor = feature_extractor\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        # Don't use MolStandardize since it's not available in this version\n",
    "        # We'll implement our own simple standardization function\n",
    "            \n",
    "    def standardize_mol(self, mol):\n",
    "        \"\"\"Standardize a molecule to ensure consistency\"\"\"\n",
    "        if mol is None:\n",
    "            return None\n",
    "        try:\n",
    "            # Basic standardization\n",
    "            mol = Chem.RemoveHs(mol)\n",
    "            Chem.SanitizeMol(mol)\n",
    "            return mol\n",
    "        except:\n",
    "            return None\n",
    "            \n",
    "    def add_remove_hydrogen(self, mol):\n",
    "        \"\"\"Add or remove hydrogen atoms from the molecule\"\"\"\n",
    "        if mol is None:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # 50% chance to add hydrogens, 50% chance to remove them\n",
    "            if random.random() < 0.5:\n",
    "                # Add hydrogens\n",
    "                mol = Chem.AddHs(mol)\n",
    "            else:\n",
    "                # Remove hydrogens\n",
    "                mol = Chem.RemoveHs(mol)\n",
    "                \n",
    "            return mol\n",
    "        except:\n",
    "            return mol\n",
    "            \n",
    "    def change_bond_order(self, mol):\n",
    "        \"\"\"Change the order of a random bond (single↔double, triple↔double)\"\"\"\n",
    "        if mol is None:\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            # Make a copy to avoid modifying the original\n",
    "            mol_copy = Chem.Mol(mol)\n",
    "            \n",
    "            # Get all bonds\n",
    "            bonds = list(mol_copy.GetBonds())\n",
    "            if not bonds:\n",
    "                return mol_copy\n",
    "                \n",
    "            # Select a random bond\n",
    "            bond = random.choice(bonds)\n",
    "            bond_idx = bond.GetIdx()\n",
    "            bond_type = bond.GetBondType()\n",
    "            \n",
    "            # Change bond type\n",
    "            edit_mol = Chem.EditableMol(mol_copy)\n",
    "            \n",
    "            if bond_type == Chem.BondType.SINGLE:\n",
    "                edit_mol.ReplaceBond(bond_idx, Chem.BondType.DOUBLE)\n",
    "            elif bond_type == Chem.BondType.DOUBLE:\n",
    "                # 50% chance to go to single, 50% to triple\n",
    "                new_type = random.choice([Chem.BondType.SINGLE, Chem.BondType.TRIPLE])\n",
    "                edit_mol.ReplaceBond(bond_idx, new_type)\n",
    "            elif bond_type == Chem.BondType.TRIPLE:\n",
    "                edit_mol.ReplaceBond(bond_idx, Chem.BondType.DOUBLE)\n",
    "            \n",
    "            mod_mol = edit_mol.GetMol()\n",
    "            \n",
    "            # Check if the molecule is valid\n",
    "            try:\n",
    "                Chem.SanitizeMol(mod_mol)\n",
    "                return mod_mol\n",
    "            except:\n",
    "                return mol_copy\n",
    "        except:\n",
    "            return mol\n",
    "            \n",
    "    def enumerate_stereoisomer(self, mol):\n",
    "        \"\"\"Generate a random stereoisomer of the molecule\"\"\"\n",
    "        if mol is None:\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            # Set up stereoisomer options\n",
    "            opts = StereoEnumerationOptions(tryEmbedding=True, maxIsomers=5)\n",
    "            \n",
    "            # Get all possible stereoisomers\n",
    "            isomers = list(EnumerateStereoisomers(mol, options=opts))\n",
    "            \n",
    "            if isomers:\n",
    "                # Return a random stereoisomer\n",
    "                return random.choice(isomers)\n",
    "            else:\n",
    "                return mol\n",
    "        except:\n",
    "            return mol\n",
    "            \n",
    "    def mutate_functional_group(self, mol):\n",
    "        \"\"\"Replace a functional group with another similar one\"\"\"\n",
    "        if mol is None:\n",
    "            return None\n",
    "            \n",
    "        # Define some functional group transformations\n",
    "        transformations = [\n",
    "            # Alcohol to ether\n",
    "            ('[OH]', '[OC]'),\n",
    "            # Carboxylic acid to ester\n",
    "            ('C(=O)[OH]', 'C(=O)OC'),\n",
    "            # Amide to ester\n",
    "            ('C(=O)[NH2]', 'C(=O)OC'),\n",
    "            # Amine to amide\n",
    "            ('[NH2]', '[NH]C(=O)C'),\n",
    "            # Ketone to aldehyde\n",
    "            ('C(=O)C', 'C(=O)[H]'),\n",
    "            # Ether to alcohol\n",
    "            ('COC', 'CO'),\n",
    "            # Nitro to amine\n",
    "            ('[N+](=O)[O-]', '[NH2]')\n",
    "        ]\n",
    "        \n",
    "        try:\n",
    "            # Choose a random transformation\n",
    "            old_pattern, new_pattern = random.choice(transformations)\n",
    "            \n",
    "            # Apply the transformation using SMILES replacement\n",
    "            smiles = Chem.MolToSmiles(mol)\n",
    "            old_pattern_mol = Chem.MolFromSmarts(old_pattern)\n",
    "            \n",
    "            if mol.HasSubstructMatch(old_pattern_mol):\n",
    "                # Create the replacement\n",
    "                modified_smiles = smiles.replace(old_pattern, new_pattern, 1)\n",
    "                new_mol = Chem.MolFromSmiles(modified_smiles)\n",
    "                \n",
    "                if new_mol is not None:\n",
    "                    return new_mol\n",
    "            \n",
    "            return mol\n",
    "        except:\n",
    "            return mol\n",
    "            \n",
    "    def change_atom(self, mol):\n",
    "        \"\"\"Substitute an atom with another from the same group\"\"\"\n",
    "        if mol is None:\n",
    "            return None\n",
    "            \n",
    "        # Define atom replacements (elements from same group tend to have similar properties)\n",
    "        replacements = {\n",
    "            'C': ['Si'],  # Carbon -> Silicon\n",
    "            'N': ['P'],   # Nitrogen -> Phosphorus\n",
    "            'O': ['S'],   # Oxygen -> Sulfur\n",
    "            'F': ['Cl', 'Br'],  # Halogen replacements\n",
    "            'Cl': ['F', 'Br'],\n",
    "            'Br': ['F', 'Cl'],\n",
    "            'S': ['O', 'Se']   # Sulfur -> Oxygen or Selenium\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Make an editable copy\n",
    "            edit_mol = Chem.EditableMol(Chem.Mol(mol))\n",
    "            atoms = list(mol.GetAtoms())\n",
    "            \n",
    "            if not atoms:\n",
    "                return mol\n",
    "                \n",
    "            # Select a random atom\n",
    "            atom = random.choice(atoms)\n",
    "            symbol = atom.GetSymbol()\n",
    "            \n",
    "            # Check if we have a replacement for this atom\n",
    "            if symbol in replacements and random.random() < 0.7:  # 70% chance to replace\n",
    "                new_symbol = random.choice(replacements[symbol])\n",
    "                new_atomic_num = Chem.GetPeriodicTable().GetAtomicNumber(new_symbol)\n",
    "                \n",
    "                # Replace the atom\n",
    "                edit_mol.ReplaceAtom(atom.GetIdx(), Chem.Atom(new_atomic_num))\n",
    "                \n",
    "                new_mol = edit_mol.GetMol()\n",
    "                \n",
    "                # Check if valid\n",
    "                try:\n",
    "                    Chem.SanitizeMol(new_mol)\n",
    "                    return new_mol\n",
    "                except:\n",
    "                    return mol\n",
    "            \n",
    "            return mol\n",
    "        except:\n",
    "            return mol\n",
    "    \n",
    "    def add_ring_substituent(self, mol):\n",
    "        \"\"\"Add a small substituent to a ring\"\"\"\n",
    "        if mol is None:\n",
    "            return None\n",
    "            \n",
    "        # Common substituents\n",
    "        substituents = [\n",
    "            ('c1ccccc1', 'c1ccccc1[C]'),  # Add methyl to benzene\n",
    "            ('c1ccccc1', 'c1ccccc1[O]'),  # Add hydroxyl to benzene\n",
    "            ('c1ccccc1', 'c1ccccc1[F]'),  # Add fluorine to benzene\n",
    "            ('c1ccccc1', 'c1ccccc1[Cl]'),  # Add chlorine to benzene\n",
    "            ('c1ccccc1', 'c1ccccc1[N]')   # Add amino to benzene\n",
    "        ]\n",
    "        \n",
    "        try:\n",
    "            # Choose a random substituent\n",
    "            old_pattern, new_pattern = random.choice(substituents)\n",
    "            \n",
    "            # Apply the transformation using SMILES replacement\n",
    "            smiles = Chem.MolToSmiles(mol)\n",
    "            old_pattern_mol = Chem.MolFromSmarts(old_pattern)\n",
    "            \n",
    "            if mol.HasSubstructMatch(old_pattern_mol):\n",
    "                # Create the replacement\n",
    "                modified_smiles = smiles.replace(old_pattern, new_pattern, 1)\n",
    "                new_mol = Chem.MolFromSmiles(modified_smiles)\n",
    "                \n",
    "                if new_mol is not None:\n",
    "                    return new_mol\n",
    "            \n",
    "            return mol\n",
    "        except:\n",
    "            return mol\n",
    "    \n",
    "    def apply_random_augmentation(self, mol):\n",
    "        \"\"\"Apply a random augmentation from the available methods\"\"\"\n",
    "        if mol is None:\n",
    "            return None\n",
    "            \n",
    "        # Define augmentation methods with their probabilities\n",
    "        augmentations = [\n",
    "            (self.add_remove_hydrogen, 0.1),\n",
    "            (self.change_bond_order, 0.2),\n",
    "            (self.enumerate_stereoisomer, 0.15),\n",
    "            (self.mutate_functional_group, 0.25),\n",
    "            (self.change_atom, 0.2),\n",
    "            (self.add_ring_substituent, 0.1)\n",
    "        ]\n",
    "        \n",
    "        # Normalize probabilities\n",
    "        total_prob = sum(prob for _, prob in augmentations)\n",
    "        normalized_probs = [prob / total_prob for _, prob in augmentations]\n",
    "        \n",
    "        # Select augmentation based on probability\n",
    "        aug_func = np.random.choice([aug for aug, _ in augmentations], p=normalized_probs)\n",
    "        \n",
    "        # Apply the selected augmentation\n",
    "        augmented_mol = aug_func(mol)\n",
    "        \n",
    "        # Standardize the result\n",
    "        return self.standardize_mol(augmented_mol)\n",
    "    \n",
    "    def generate_augmented_dataset(self, num_augmentations=1):  \n",
    "        \"\"\"Generate augmented dataset from original SMILES\"\"\"\n",
    "        augmented_data_list = []\n",
    "\n",
    "        for i, smiles in enumerate(tqdm(self.smiles_list, desc=\"Generating augmentations\")):\n",
    "            # Convert to molecule\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "\n",
    "            if mol is None:\n",
    "                continue\n",
    "\n",
    "            # Add the original molecule\n",
    "            if self.feature_extractor is not None:\n",
    "                original_data = self.feature_extractor.process_molecule(smiles)\n",
    "                if original_data is not None:\n",
    "                    # Store original SMILES as attribute\n",
    "                    original_data.smiles = smiles\n",
    "                    # Add the original_idx attribute to ALL objects for consistency\n",
    "                    original_data.original_idx = i\n",
    "                    augmented_data_list.append(original_data)\n",
    "\n",
    "            # Generate augmentations\n",
    "            augmented_mol = self.apply_random_augmentation(mol)\n",
    "\n",
    "            if augmented_mol is not None:\n",
    "                # Convert back to SMILES\n",
    "                aug_smiles = Chem.MolToSmiles(augmented_mol)\n",
    "\n",
    "                # Convert to graph data\n",
    "                if self.feature_extractor is not None:\n",
    "                    aug_data = self.feature_extractor.process_molecule(aug_smiles)\n",
    "                    if aug_data is not None:\n",
    "                        # Store SMILES as attribute\n",
    "                        aug_data.smiles = aug_smiles\n",
    "                        # Store the original molecule index for reference\n",
    "                        aug_data.original_idx = i\n",
    "                        augmented_data_list.append(aug_data)\n",
    "\n",
    "        return augmented_data_list\n",
    "\n",
    "def train_encoder_with_manual_augmentations(dataset, feature_extractor, output_dir='./embeddings', \n",
    "                                           batch_size=32, epochs=50, lr=1e-4, hidden_dim=128, \n",
    "                                           output_dim=128, device='cuda'):\n",
    "    \"\"\"Train encoder using manual augmentations\"\"\"\n",
    "    from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    import torch.optim as optim\n",
    "    from torch_geometric.data import Batch\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Extract SMILES strings for augmentation\n",
    "    smiles_list = []\n",
    "    for i, data in enumerate(dataset):\n",
    "        mol_id = f\"molecule_{i}\"\n",
    "        # If you have SMILES in the dataset, extract them\n",
    "        if hasattr(data, 'smiles'):\n",
    "            smiles_list.append(data.smiles)\n",
    "        else:\n",
    "            # Otherwise, try to convert PyG data to SMILES using RDKit\n",
    "            smiles = convert_pyg_to_smiles(data)\n",
    "            smiles_list.append(smiles)\n",
    "    \n",
    "    # Create augmenter and generate augmented dataset\n",
    "    augmenter = MolecularAugmenter(smiles_list, feature_extractor)\n",
    "    augmented_dataset = augmenter.generate_augmented_dataset(num_augmentations=1)\n",
    "    \n",
    "    print(f\"Generated {len(augmented_dataset)} augmented molecules (including originals)\")\n",
    "    \n",
    "    # Define custom collate function to handle additional attributes\n",
    "    def custom_collate(data_list):\n",
    "        keys_to_exclude = ['smiles', 'original_idx']  # Add any custom attributes here\n",
    "        \n",
    "        # Store the excluded attributes\n",
    "        excluded_data = {key: [getattr(data, key, None) for data in data_list] for key in keys_to_exclude}\n",
    "        \n",
    "        # Create batch without excluded keys\n",
    "        batch = Batch.from_data_list(data_list)\n",
    "        \n",
    "        # Add back the excluded attributes as lists\n",
    "        for key, values in excluded_data.items():\n",
    "            setattr(batch, key, values)\n",
    "            \n",
    "        return batch\n",
    "    \n",
    "    # Define the encoder model\n",
    "    class GraphEncoder(nn.Module):\n",
    "        def __init__(self, node_dim, edge_dim, hidden_dim=128, output_dim=128):\n",
    "            super().__init__()\n",
    "            \n",
    "            # Feature encoding layers\n",
    "            self.node_encoder = nn.Sequential(\n",
    "                nn.Linear(node_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim)\n",
    "            )\n",
    "            \n",
    "            self.edge_encoder = nn.Sequential(\n",
    "                nn.Linear(edge_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim)\n",
    "            )\n",
    "            \n",
    "            # Graph convolution layers\n",
    "            self.conv1 = GCNConv(hidden_dim, hidden_dim)\n",
    "            self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "            self.conv3 = GCNConv(hidden_dim, output_dim)\n",
    "            \n",
    "            # Projection head for contrastive learning\n",
    "            self.projection = nn.Sequential(\n",
    "                nn.Linear(output_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, output_dim)\n",
    "            )\n",
    "        \n",
    "        def normalize_features(self, x_cat, x_phys):\n",
    "            \"\"\"Normalize categorical and physical features separately\"\"\"\n",
    "            # Convert categorical features to one-hot\n",
    "            x_cat = x_cat.float()\n",
    "            \n",
    "            # Normalize physical features\n",
    "            x_phys = x_phys.float()\n",
    "            if x_phys.size(0) > 1:  # Only normalize if we have more than one sample\n",
    "                x_phys = (x_phys - x_phys.mean(0)) / (x_phys.std(0) + 1e-5)\n",
    "                \n",
    "            return x_cat, x_phys\n",
    "        \n",
    "        def forward(self, data):\n",
    "            # Normalize features\n",
    "            x_cat, x_phys = self.normalize_features(data.x_cat, data.x_phys)\n",
    "            \n",
    "            # Concatenate features\n",
    "            x = torch.cat([x_cat, x_phys], dim=-1)\n",
    "            \n",
    "            edge_index = data.edge_index\n",
    "            edge_attr = data.edge_attr.float()\n",
    "            batch = data.batch\n",
    "            \n",
    "            # Initial feature encoding\n",
    "            x = self.node_encoder(x)\n",
    "            edge_attr = self.edge_encoder(edge_attr)\n",
    "            \n",
    "            # Graph convolutions\n",
    "            x = F.relu(self.conv1(x, edge_index))\n",
    "            x = F.relu(self.conv2(x, edge_index))\n",
    "            x = self.conv3(x, edge_index)\n",
    "            \n",
    "            # Global pooling\n",
    "            x = global_mean_pool(x, batch)\n",
    "            \n",
    "            # Projection\n",
    "            x = self.projection(x)\n",
    "            \n",
    "            return x\n",
    "    \n",
    "    # Create data loader with custom collate function\n",
    "    train_loader = DataLoader(augmented_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate)\n",
    "    \n",
    "    # Calculate input dimensions\n",
    "    sample_data = augmented_dataset[0]\n",
    "    node_dim = sample_data.x_cat.shape[1] + sample_data.x_phys.shape[1]\n",
    "    edge_dim = sample_data.edge_attr.shape[1]\n",
    "    \n",
    "    # Initialize model\n",
    "    model = GraphEncoder(node_dim, edge_dim, hidden_dim, output_dim).to(device)\n",
    "    \n",
    "    # Define optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Define contrastive loss function\n",
    "    def contrastive_loss(query, key, temperature=0.07):\n",
    "        # Normalize embeddings\n",
    "        query = F.normalize(query, dim=1)\n",
    "        key = F.normalize(key, dim=1)\n",
    "        \n",
    "        # Calculate similarity\n",
    "        logits = torch.mm(query, key.T) / temperature\n",
    "        \n",
    "        # Set labels to be the positive samples (diagonal elements)\n",
    "        labels = torch.arange(logits.shape[0], device=device)\n",
    "        \n",
    "        return F.cross_entropy(logits, labels)\n",
    "    \n",
    "    # Training loop\n",
    "    print(\"Training encoder with manual augmentations...\")\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            batch = batch.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            embeddings = model(batch)\n",
    "            \n",
    "            # Create positive pairs by splitting the batch\n",
    "            batch_size = embeddings.size(0)\n",
    "            half_size = batch_size // 2\n",
    "            \n",
    "            if half_size > 0:\n",
    "                # Split embeddings\n",
    "                query_emb = embeddings[:half_size]\n",
    "                key_emb = embeddings[half_size:2*half_size]\n",
    "                \n",
    "                # Calculate loss for this mini-batch\n",
    "                loss = contrastive_loss(query_emb, key_emb)\n",
    "                \n",
    "                # Backward pass and optimization\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                epoch_loss += loss.item()\n",
    "        \n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch+1}, Avg Loss: {avg_loss:.4f}')\n",
    "        \n",
    "        # Save checkpoint every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': avg_loss,\n",
    "            }, os.path.join(output_dir, f'manual_encoder_checkpoint_{epoch+1}.pt'))\n",
    "    \n",
    "    # Extract embeddings for the original dataset\n",
    "    model.eval()\n",
    "    \n",
    "    # Create a custom collate function for the original dataset too\n",
    "    def original_collate(data_list):\n",
    "        keys_to_exclude = ['smiles']  # Only exclude 'smiles' for original data\n",
    "        \n",
    "        # Store the excluded attributes\n",
    "        excluded_data = {key: [getattr(data, key, None) for data in data_list] for key in keys_to_exclude}\n",
    "        \n",
    "        # Create batch without excluded keys\n",
    "        batch = Batch.from_data_list(data_list)\n",
    "        \n",
    "        # Add back the excluded attributes as lists\n",
    "        for key, values in excluded_data.items():\n",
    "            setattr(batch, key, values)\n",
    "            \n",
    "        return batch\n",
    "    \n",
    "    original_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=original_collate)\n",
    "    all_embeddings = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(original_loader, desc=\"Extracting embeddings\"):\n",
    "            batch = batch.to(device)\n",
    "            embeddings = model(batch)\n",
    "            all_embeddings.append(embeddings.cpu().numpy())\n",
    "    \n",
    "    all_embeddings = np.vstack(all_embeddings)\n",
    "    \n",
    "    # Save the embeddings\n",
    "    embeddings_data = {\n",
    "        'embeddings': all_embeddings,\n",
    "        'molecule_data': [{\n",
    "            'smiles': getattr(data, 'smiles', ''),\n",
    "            'num_nodes': data.num_nodes,\n",
    "            'edge_index': data.edge_index.tolist() if hasattr(data, 'edge_index') else None,\n",
    "            'x_cat': data.x_cat.tolist() if hasattr(data, 'x_cat') else None,\n",
    "            'x_phys': data.x_phys.tolist() if hasattr(data, 'x_phys') else None,\n",
    "            'edge_attr': data.edge_attr.tolist() if hasattr(data, 'edge_attr') else None\n",
    "        } for data in dataset]\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(output_dir, 'manual_embeddings.pkl'), 'wb') as f:\n",
    "        pickle.dump(embeddings_data, f)\n",
    "    \n",
    "    print(f\"Manual embeddings saved to {os.path.join(output_dir, 'manual_embeddings.pkl')}\")\n",
    "    \n",
    "    return model, all_embeddings\n",
    "\n",
    "def convert_pyg_to_smiles(pyg_data):\n",
    "    \"\"\"Attempt to convert PyG data to SMILES string using RDKit\"\"\"\n",
    "    try:\n",
    "        # Extract node features and edge information\n",
    "        x_cat = pyg_data.x_cat.numpy() if hasattr(pyg_data, 'x_cat') else None\n",
    "        edge_index = pyg_data.edge_index.numpy() if hasattr(pyg_data, 'edge_index') else None\n",
    "        \n",
    "        if x_cat is None or edge_index is None:\n",
    "            return ''\n",
    "        \n",
    "        # Create empty RDKit molecule\n",
    "        mol = Chem.RWMol()\n",
    "        \n",
    "        # Add atoms (assuming first column of x_cat is atomic number)\n",
    "        for atom_feat in x_cat:\n",
    "            atom_num = int(atom_feat[0])\n",
    "            atom = Chem.Atom(atom_num)\n",
    "            mol.AddAtom(atom)\n",
    "        \n",
    "        # Add bonds (assuming edges represent single bonds for simplicity)\n",
    "        for i in range(edge_index.shape[1]):\n",
    "            src, dst = edge_index[0, i], edge_index[1, i]\n",
    "            # Avoid adding duplicate bonds (only add if src < dst)\n",
    "            if src < dst:\n",
    "                mol.AddBond(int(src), int(dst), Chem.BondType.SINGLE)\n",
    "        \n",
    "        # Convert to molecule and get SMILES\n",
    "        mol = mol.GetMol()\n",
    "        return Chem.MolToSmiles(mol)\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting PyG data to SMILES: {e}\")\n",
    "        return ''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78ad54e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_molecule_metadata(dataset):\n",
    "    \"\"\"Extract metadata from PyG graph data without relying on SMILES strings\"\"\"\n",
    "    from rdkit import Chem\n",
    "    from rdkit.Chem import Descriptors\n",
    "    from tqdm import tqdm\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    import networkx as nx\n",
    "    from collections import defaultdict\n",
    "    \n",
    "    metadata = []\n",
    "    \n",
    "    for i, data in enumerate(tqdm(dataset, desc=\"Extracting molecule metadata\")):\n",
    "        # Set graph ID\n",
    "        mol_id = f\"molecule_{i}\"\n",
    "        \n",
    "        # Initialize empty dictionaries for metadata\n",
    "        properties = {}\n",
    "        features = {}\n",
    "        functional_groups = {}\n",
    "        ring_info = {\"ring_counts\": {}, \"ring_sizes\": {}}\n",
    "        \n",
    "        # Extract basic graph properties directly from the PyG data\n",
    "        if hasattr(data, 'num_nodes') and hasattr(data, 'edge_index'):\n",
    "            try:\n",
    "                # Convert to networkx graph for analysis\n",
    "                G = to_networkx(data)\n",
    "                \n",
    "                # Calculate graph-level properties\n",
    "                num_edges = data.edge_index.size(1) // 2  # Count unique edges\n",
    "                properties = {\n",
    "                    \"num_nodes\": data.num_nodes,\n",
    "                    \"num_edges\": num_edges,\n",
    "                    \"avg_node_degree\": 2 * num_edges / data.num_nodes if data.num_nodes > 0 else 0\n",
    "                }\n",
    "                \n",
    "                # Calculate average path length if graph is connected\n",
    "                if nx.is_connected(G):\n",
    "                    try:\n",
    "                        properties[\"avg_path_length\"] = nx.average_shortest_path_length(G)\n",
    "                    except:\n",
    "                        properties[\"avg_path_length\"] = 0.0\n",
    "                else:\n",
    "                    properties[\"avg_path_length\"] = 0.0\n",
    "                \n",
    "                # Add more sophisticated graph properties\n",
    "                try:\n",
    "                    properties[\"clustering_coefficient\"] = nx.average_clustering(G)\n",
    "                except:\n",
    "                    properties[\"clustering_coefficient\"] = 0.0\n",
    "                \n",
    "                try:\n",
    "                    properties[\"graph_diameter\"] = nx.diameter(G) if nx.is_connected(G) else 0\n",
    "                except:\n",
    "                    properties[\"graph_diameter\"] = 0\n",
    "\n",
    "                try:\n",
    "                    properties[\"assortativity\"] = nx.degree_assortativity_coefficient(G)\n",
    "                except:\n",
    "                    properties[\"assortativity\"] = 0.0\n",
    "                \n",
    "                # Graph features\n",
    "                features = {\n",
    "                    \"is_connected\": nx.is_connected(G),\n",
    "                    \"num_connected_components\": nx.number_connected_components(G),\n",
    "                    \"has_cycles\": not nx.is_tree(G),\n",
    "                    \"max_degree\": max(dict(G.degree()).values()) if G.number_of_nodes() > 0 else 0,\n",
    "                    \"density\": nx.density(G),\n",
    "                    \"is_bipartite\": nx.is_bipartite(G) if G.number_of_nodes() > 0 else False\n",
    "                }\n",
    "                \n",
    "                # Get centrality measures\n",
    "                if G.number_of_nodes() > 0:\n",
    "                    try:\n",
    "                        degree_centrality = nx.degree_centrality(G)\n",
    "                        features[\"max_centrality\"] = max(degree_centrality.values()) if degree_centrality else 0\n",
    "                        features[\"avg_centrality\"] = sum(degree_centrality.values()) / len(degree_centrality) if degree_centrality else 0\n",
    "                    except:\n",
    "                        features[\"max_centrality\"] = 0\n",
    "                        features[\"avg_centrality\"] = 0\n",
    "                else:\n",
    "                    features[\"max_centrality\"] = 0\n",
    "                    features[\"avg_centrality\"] = 0\n",
    "                \n",
    "                # Analyze node features if available\n",
    "                if hasattr(data, 'x_cat') and hasattr(data, 'x_phys'):\n",
    "                    # Atomic element distribution (from x_cat)\n",
    "                    atom_types = {}\n",
    "                    if data.x_cat.size(1) > 0:\n",
    "                        for i in range(data.num_nodes):\n",
    "                            atom_type = int(data.x_cat[i, 0].item())\n",
    "                            atom_types[atom_type] = atom_types.get(atom_type, 0) + 1\n",
    "                    \n",
    "                    features[\"atom_type_distribution\"] = atom_types\n",
    "                    \n",
    "                    # Physical property statistics (from x_phys)\n",
    "                    if data.x_phys.size(1) > 0:\n",
    "                        phys_means = data.x_phys.mean(dim=0).tolist() \n",
    "                        phys_stds = data.x_phys.std(dim=0).tolist()\n",
    "                        \n",
    "                        # Map indices to meaningful property names for the first few common properties\n",
    "                        phys_prop_names = ['contrib_mw', 'contrib_logp', 'formal_charge', \n",
    "                                        'hybridization', 'is_aromatic', 'num_h', 'valence', 'degree']\n",
    "                        \n",
    "                        for idx, name in enumerate(phys_prop_names):\n",
    "                            if idx < len(phys_means):\n",
    "                                properties[f\"avg_{name}\"] = phys_means[idx]\n",
    "                                properties[f\"std_{name}\"] = phys_stds[idx]\n",
    "                \n",
    "                # Cycle analysis\n",
    "                cycles = list(nx.cycle_basis(G))\n",
    "                cycle_count = len(cycles)\n",
    "                ring_info[\"ring_counts\"][\"total\"] = cycle_count\n",
    "                \n",
    "                # Count rings by size\n",
    "                ring_sizes = defaultdict(int)\n",
    "                for cycle in cycles:\n",
    "                    size = len(cycle)\n",
    "                    ring_sizes[str(size)] = ring_sizes.get(str(size), 0) + 1\n",
    "                \n",
    "                # Ensure we have entries for common ring sizes\n",
    "                for size in range(3, 11):\n",
    "                    if str(size) not in ring_sizes:\n",
    "                        ring_sizes[str(size)] = 0\n",
    "                \n",
    "                ring_info[\"ring_sizes\"] = dict(ring_sizes)\n",
    "                \n",
    "                # Estimate ring types\n",
    "                ring_info[\"ring_counts\"][\"single\"] = 0\n",
    "                ring_info[\"ring_counts\"][\"fused\"] = 0\n",
    "                \n",
    "                # Identify single vs fused rings by checking for shared nodes\n",
    "                if cycles:\n",
    "                    # Build a mapping of nodes to cycles they belong to\n",
    "                    node_to_cycles = defaultdict(list)\n",
    "                    for cycle_idx, cycle in enumerate(cycles):\n",
    "                        for node in cycle:\n",
    "                            node_to_cycles[node].append(cycle_idx)\n",
    "                    \n",
    "                    # Count single rings (no shared nodes with other rings)\n",
    "                    shared_cycles = set()\n",
    "                    for node, cycle_list in node_to_cycles.items():\n",
    "                        if len(cycle_list) > 1:\n",
    "                            for c in cycle_list:\n",
    "                                shared_cycles.add(c)\n",
    "                    \n",
    "                    ring_info[\"ring_counts\"][\"single\"] = cycle_count - len(shared_cycles)\n",
    "                    ring_info[\"ring_counts\"][\"fused\"] = len(shared_cycles)\n",
    "                \n",
    "                # Edge feature analysis if available\n",
    "                if hasattr(data, 'edge_attr') and data.edge_attr.size(0) > 0:\n",
    "                    # Analyze bond types (assuming first dimension is bond type)\n",
    "                    bond_types = {}\n",
    "                    for i in range(data.edge_attr.size(0)):\n",
    "                        if data.edge_attr.size(1) > 0:\n",
    "                            bond_type = int(data.edge_attr[i, 0].item())\n",
    "                            bond_types[bond_type] = bond_types.get(bond_type, 0) + 1\n",
    "                    \n",
    "                    # Divide by 2 since each bond is counted twice in undirected graph\n",
    "                    for bt in bond_types:\n",
    "                        bond_types[bt] = bond_types[bt] // 2\n",
    "                    \n",
    "                    functional_groups[\"bond_types\"] = bond_types\n",
    "                    \n",
    "                    # Count functional group proxies based on patterns in the graph\n",
    "                    # This is just an estimate since we don't have chemical information\n",
    "                    conjugated_bonds = 0\n",
    "                    for i in range(data.edge_attr.size(0)):\n",
    "                        if data.edge_attr.size(1) > 1 and data.edge_attr[i, 2].item() > 0:  # IsConjugated flag\n",
    "                            conjugated_bonds += 1\n",
    "                    \n",
    "                    functional_groups[\"conjugated_bonds\"] = conjugated_bonds // 2\n",
    "                    \n",
    "            except Exception as e:\n",
    "                # If any error occurs during analysis, use minimal information\n",
    "                print(f\"Error analyzing graph {i}: {e}\")\n",
    "        \n",
    "        metadata.append({\n",
    "            \"graph_id\": mol_id,\n",
    "            \"properties\": properties,\n",
    "            \"features\": features,\n",
    "            \"functional_groups\": functional_groups,\n",
    "            \"ring_info\": ring_info\n",
    "        })\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "def to_networkx(data):\n",
    "    \"\"\"Convert PyG data to networkx graph for analysis\"\"\"\n",
    "    import networkx as nx\n",
    "    \n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add nodes\n",
    "    for i in range(data.num_nodes):\n",
    "        G.add_node(i)\n",
    "    \n",
    "    # Add edges (removing duplicates and self-loops)\n",
    "    edge_index = data.edge_index.cpu().numpy()\n",
    "    edges = set()\n",
    "    for i in range(edge_index.shape[1]):\n",
    "        u, v = edge_index[0, i], edge_index[1, i]\n",
    "        if u != v and (u, v) not in edges and (v, u) not in edges:\n",
    "            G.add_edge(u, v)\n",
    "            edges.add((u, v))\n",
    "    \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e90813e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embedding_file(embeddings, molecule_indices, training_info, model_config, filepath):\n",
    "    \"\"\"Save embeddings with training metadata\"\"\"\n",
    "    data = {\n",
    "        \"embeddings\": embeddings,\n",
    "        \"molecule_indices\": molecule_indices,\n",
    "        \"training_info\": training_info,\n",
    "        \"model_config\": {k: v for k, v in model_config.__dict__.items() \n",
    "                         if not k.startswith('_') and not callable(v)}\n",
    "    }\n",
    "    \n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "        \n",
    "def save_embeddings_with_molecules(embeddings, dataset, filepath):\n",
    "    \"\"\"Save embeddings with corresponding molecule information and graph-level properties\"\"\"\n",
    "    # Create a list to store molecule data\n",
    "    molecule_data = []\n",
    "    \n",
    "    # Extract important info from each molecule in the dataset\n",
    "    for data in dataset:\n",
    "        # Create a dictionary with basic graph properties\n",
    "        mol_info = {\n",
    "            \"num_nodes\": data.num_nodes,\n",
    "            \"edge_index\": data.edge_index.tolist() if hasattr(data, 'edge_index') else None,\n",
    "            \"x_cat\": data.x_cat.tolist() if hasattr(data, 'x_cat') else None,\n",
    "            \"x_phys\": data.x_phys.tolist() if hasattr(data, 'x_phys') else None,\n",
    "            \"edge_attr\": data.edge_attr.tolist() if hasattr(data, 'edge_attr') else None,\n",
    "            \"smiles\": data.smiles if hasattr(data, 'smiles') else \"\"  # This is missing in your code\n",
    "        }\n",
    "        \n",
    "        # Calculate additional graph properties if possible\n",
    "        try:\n",
    "            if hasattr(data, 'edge_index') and hasattr(data, 'num_nodes'):\n",
    "                # Graph density\n",
    "                num_edges = len(data.edge_index[0]) // 2  # Undirected edges counted once\n",
    "                max_edges = data.num_nodes * (data.num_nodes - 1) // 2\n",
    "                density = num_edges / max_edges if max_edges > 0 else 0\n",
    "                mol_info[\"graph_density\"] = density\n",
    "                \n",
    "                # Average degree\n",
    "                avg_degree = num_edges * 2 / data.num_nodes if data.num_nodes > 0 else 0\n",
    "                mol_info[\"avg_degree\"] = avg_degree\n",
    "                \n",
    "                # Count atom types if available\n",
    "                if hasattr(data, 'x_cat') and data.x_cat is not None:\n",
    "                    atom_types = {}\n",
    "                    for atom in data.x_cat:\n",
    "                        atom_type = int(atom[0])\n",
    "                        atom_types[atom_type] = atom_types.get(atom_type, 0) + 1\n",
    "                    mol_info[\"atom_type_counts\"] = atom_types\n",
    "                \n",
    "                # Count bond types if available\n",
    "                if hasattr(data, 'edge_attr') and data.edge_attr is not None:\n",
    "                    bond_types = {}\n",
    "                    for bond in data.edge_attr:\n",
    "                        bond_type = int(bond[0])\n",
    "                        bond_types[bond_type] = bond_types.get(bond_type, 0) + 1\n",
    "                    mol_info[\"bond_type_counts\"] = bond_types\n",
    "        except:\n",
    "            # If calculation fails, continue without these properties\n",
    "            pass\n",
    "            \n",
    "        molecule_data.append(mol_info)\n",
    "    \n",
    "    # Save both embeddings and molecule data\n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'embeddings': embeddings,\n",
    "            'molecule_data': molecule_data,\n",
    "            'graph_properties': True,  # Flag to indicate enhanced properties are stored\n",
    "            'smiles_list': [data.smiles for data in dataset if hasattr(data, 'smiles')] # This is missing\n",
    "        }, f)\n",
    "    \n",
    "    print(f\"Saved embeddings and molecule data with graph properties to {filepath}\")\n",
    "        \n",
    "\n",
    "def save_embeddings(embeddings, labels, filepath):\n",
    "    \"\"\"Save embeddings and corresponding labels\"\"\"\n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'embeddings': embeddings,\n",
    "            'labels': labels\n",
    "        }, f)\n",
    "\n",
    "def save_encoder(encoder, save_path, info=None):\n",
    "    \"\"\"Save encoder model for downstream tasks\"\"\"\n",
    "    save_dict = {\n",
    "        'encoder_state_dict': encoder.state_dict(),\n",
    "        'model_info': info or {}\n",
    "    }\n",
    "    torch.save(save_dict, save_path)\n",
    "\n",
    "def load_encoder(model_path, device='cpu'):\n",
    "    \"\"\"Load saved encoder model\"\"\"\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    encoder = GraphDiscriminator(\n",
    "        node_dim=checkpoint['model_info'].get('node_dim'),\n",
    "        edge_dim=checkpoint['model_info'].get('edge_dim'),\n",
    "        hidden_dim=checkpoint['model_info'].get('hidden_dim', 128),\n",
    "        output_dim=checkpoint['model_info'].get('output_dim', 128)\n",
    "    )\n",
    "    encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "    return encoder        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dbc0517",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data loading...\n",
      "1. Loaded dataset with 41 graphs.\n",
      "2. Failed SMILES count: 0\n",
      "3. Using device: cpu\n",
      "4. Creating molecular augmenter...\n",
      "5. Generating augmented molecules...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|████████████████████████████████████████████████████████| 41/41 [00:05<00:00,  7.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   - Generated 82 molecules (including originals)\n",
      "6. Training encoder with manual augmentations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|████████████████████████████████████████████████████████| 41/41 [00:05<00:00,  7.42it/s]\n",
      "C:\\Users\\Malli\\anaconda3\\envs\\baceenv\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 82 augmented molecules (including originals)\n",
      "Training encoder with manual augmentations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 12.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Avg Loss: 2.5647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 15.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Avg Loss: 2.5957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 14.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Avg Loss: 2.5956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 13.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Avg Loss: 2.6001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 12.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Avg Loss: 2.5484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 12.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Avg Loss: 2.5728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 14.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Avg Loss: 2.5765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 13.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Avg Loss: 2.5940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 14.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Avg Loss: 2.5829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: 100%|███████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 12.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Avg Loss: 2.5872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: 100%|███████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 16.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Avg Loss: 2.5806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50: 100%|███████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 13.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Avg Loss: 2.5941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50: 100%|███████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 14.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Avg Loss: 2.5735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50: 100%|███████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 14.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Avg Loss: 2.5929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50: 100%|███████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 12.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Avg Loss: 2.5822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50: 100%|███████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 12.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Avg Loss: 2.5834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50: 100%|███████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 14.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Avg Loss: 2.5823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50: 100%|███████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 14.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Avg Loss: 2.5782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/50: 100%|███████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 14.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Avg Loss: 2.5806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/50: 100%|███████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 13.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Avg Loss: 2.5824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/50: 100%|███████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 13.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, Avg Loss: 2.5783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/50: 100%|███████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 13.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, Avg Loss: 2.5758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/50: 100%|███████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 14.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23, Avg Loss: 2.5869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/50: 100%|███████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 13.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24, Avg Loss: 2.5821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/50: 100%|███████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 14.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, Avg Loss: 2.5805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/50: 100%|███████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 14.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26, Avg Loss: 2.5871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/50: 100%|███████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 11.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27, Avg Loss: 2.5762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/50: 100%|███████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 14.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28, Avg Loss: 2.5806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/50: 100%|███████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 14.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29, Avg Loss: 2.5766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/50: 100%|███████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 13.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30, Avg Loss: 2.5796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/50: 100%|███████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 13.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31, Avg Loss: 2.5851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/50: 100%|███████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 13.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32, Avg Loss: 2.5735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/50: 100%|███████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 11.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33, Avg Loss: 2.5886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/50: 100%|███████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 13.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34, Avg Loss: 2.5759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/50: 100%|███████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 13.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35, Avg Loss: 2.5778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/50: 100%|███████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 14.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36, Avg Loss: 2.5752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/50: 100%|███████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 14.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37, Avg Loss: 2.5894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/50: 100%|███████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 13.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38, Avg Loss: 2.5773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/50: 100%|███████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 12.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39, Avg Loss: 2.5880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/50: 100%|███████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 13.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40, Avg Loss: 2.5914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/50: 100%|███████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 14.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41, Avg Loss: 2.5825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/50: 100%|███████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 14.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42, Avg Loss: 2.5882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/50: 100%|███████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 13.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43, Avg Loss: 2.5816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/50: 100%|███████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 12.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44, Avg Loss: 2.5738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/50: 100%|███████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 13.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45, Avg Loss: 2.5858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/50: 100%|███████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 14.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46, Avg Loss: 2.5807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/50: 100%|███████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 15.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47, Avg Loss: 2.5862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/50: 100%|███████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 13.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48, Avg Loss: 2.5834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/50: 100%|███████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 12.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49, Avg Loss: 2.5751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/50: 100%|███████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 13.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50, Avg Loss: 2.5831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings: 100%|████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 158.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual embeddings saved to ./embeddings\\manual_embeddings.pkl\n",
      "7. Training completed!\n",
      "8. Extracting molecule metadata...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting molecule metadata: 100%|███████████████████████████████████████████████████| 41/41 [00:00<00:00, 211.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9. Saving final model...\n",
      "10. Final embeddings saved to ./embeddings/final_embeddings_molecules_20250310_143038.pkl\n",
      "11. Manual encoder saved to ./checkpoints_manual\\final_manual_encoder.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Enable anomaly detection during development\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Create timestamp for file naming\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    print(\"Starting data loading...\")\n",
    "    extractor = MolecularFeatureExtractor()\n",
    "#     smiles_file = \"D:\\\\PhD\\\\Chapter3\\\\Unsupervised_GAN_Code\\\\pubchem-10m-clean_test10k.txt\"\n",
    "    smiles_file = \"D:\\\\PhD\\\\Chapter3\\\\Unsupervised_GAN_Code\\\\pubchem-41-clean.txt\"\n",
    "    \n",
    "    dataset = []\n",
    "    failed_smiles = []\n",
    "    smiles_list = []\n",
    "    \n",
    "    with open(smiles_file, 'r') as f:\n",
    "        for line in f:\n",
    "            smiles = line.strip()\n",
    "            smiles_list.append(smiles)\n",
    "            data = extractor.process_molecule(smiles)\n",
    "            if data is not None:\n",
    "                # Store SMILES as an attribute for later use\n",
    "                data.smiles = smiles\n",
    "                dataset.append(data)\n",
    "            else:\n",
    "                failed_smiles.append(smiles)\n",
    "    \n",
    "    print(f\"1. Loaded dataset with {len(dataset)} graphs.\")\n",
    "    print(f\"2. Failed SMILES count: {len(failed_smiles)}\")\n",
    "    \n",
    "    if not dataset:\n",
    "        print(\"No valid graphs generated.\")\n",
    "        return None\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"3. Using device: {device}\")\n",
    "    \n",
    "    # Create output directories\n",
    "    save_dir = './checkpoints_manual'\n",
    "    embedding_dir = './embeddings'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    os.makedirs(embedding_dir, exist_ok=True)\n",
    "    \n",
    "    # Initialize the augmenter\n",
    "    print(\"4. Creating molecular augmenter...\")\n",
    "    augmenter = MolecularAugmenter(smiles_list, extractor)\n",
    "    \n",
    "    # Generate augmented dataset\n",
    "    print(\"5. Generating augmented molecules...\")\n",
    "    augmented_dataset = augmenter.generate_augmented_dataset(num_augmentations=1)\n",
    "    print(f\"   - Generated {len(augmented_dataset)} molecules (including originals)\")\n",
    "    \n",
    "    # Setup training parameters\n",
    "    batch_size = 32\n",
    "    hidden_dim = 128\n",
    "    output_dim = 128\n",
    "    epochs = 50\n",
    "    lr = 1e-4\n",
    "    \n",
    "    # Train the encoder with manual augmentations\n",
    "    print(\"6. Training encoder with manual augmentations...\")\n",
    "    model, embeddings = train_encoder_with_manual_augmentations(\n",
    "        dataset,\n",
    "        extractor,\n",
    "        output_dir=embedding_dir,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        lr=lr,\n",
    "        hidden_dim=hidden_dim,\n",
    "        output_dim=output_dim,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    print(\"7. Training completed!\")\n",
    "    \n",
    "    # Save additional metadata\n",
    "    metadata_dir = os.path.join(embedding_dir, 'metadata')\n",
    "    os.makedirs(metadata_dir, exist_ok=True)\n",
    "    \n",
    "    # Extract and save molecule metadata\n",
    "    print(\"8. Extracting molecule metadata...\")\n",
    "    metadata = extract_molecule_metadata(dataset)\n",
    "    with open(os.path.join(metadata_dir, f'molecule_metadata_manual_{timestamp}.pkl'), 'wb') as f:\n",
    "        pickle.dump(metadata, f)\n",
    "    \n",
    "    # Save the final model\n",
    "    print(\"9. Saving final model...\")\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'config': {\n",
    "            'node_dim': model.node_encoder[0].in_features,\n",
    "            'edge_dim': model.edge_encoder[0].in_features,\n",
    "            'hidden_dim': hidden_dim,\n",
    "            'output_dim': output_dim\n",
    "        }\n",
    "    }, os.path.join(save_dir, 'final_manual_encoder.pt'))\n",
    "    \n",
    "    # Save embeddings with molecules\n",
    "    final_embedding_path = f'./embeddings/final_embeddings_molecules_{timestamp}.pkl'\n",
    "\n",
    "    # Create a dictionary with embeddings and molecule data that includes SMILES\n",
    "    embedding_data = {\n",
    "        'embeddings': embeddings,\n",
    "        'molecule_data': [{\n",
    "            'smiles': getattr(data, 'smiles', ''),  # Extract SMILES if available\n",
    "            'num_nodes': data.num_nodes,\n",
    "            'edge_index': data.edge_index.tolist() if hasattr(data, 'edge_index') else None,\n",
    "            'x_cat': data.x_cat.tolist() if hasattr(data, 'x_cat') else None,\n",
    "            'x_phys': data.x_phys.tolist() if hasattr(data, 'x_phys') else None,\n",
    "            'edge_attr': data.edge_attr.tolist() if hasattr(data, 'edge_attr') else None\n",
    "        } for data in dataset],\n",
    "        'smiles_list': [getattr(data, 'smiles', '') for data in dataset]  # Explicit SMILES list\n",
    "    }\n",
    "\n",
    "    with open(final_embedding_path, 'wb') as f:\n",
    "        pickle.dump(embedding_data, f)\n",
    "    \n",
    "    print(f\"10. Final embeddings saved to {final_embedding_path}\")\n",
    "    print(f\"11. Manual encoder saved to {os.path.join(save_dir, 'final_manual_encoder.pt')}\")\n",
    "    \n",
    "    return model, embeddings, dataset\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model, embeddings, dataset = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d928ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
