{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5379f427",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Malli\\anaconda3\\envs\\baceenv\\lib\\site-packages\\torch_geometric\\typing.py:86: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: [WinError 127] The specified procedure could not be found\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, Descriptors\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "import random\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Data\n",
    "from rdkit import RDLogger\n",
    "from rdkit.Chem import RemoveHs\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.data import DataLoader\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool, MessagePassing\n",
    "from typing import Tuple, List, Optional\n",
    "import copy\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Suppress RDKit warnings\n",
    "RDLogger.DisableLog('rdApp.warning')\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8de17bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MolecularFeatureExtractor:\n",
    "    def __init__(self):\n",
    "        self.atom_list = list(range(1, 119))\n",
    "        self.chirality_list = [\n",
    "            Chem.rdchem.ChiralType.CHI_UNSPECIFIED,\n",
    "            Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CW,\n",
    "            Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CCW,\n",
    "            Chem.rdchem.ChiralType.CHI_OTHER\n",
    "        ]\n",
    "        self.bond_list = [\n",
    "            Chem.rdchem.BondType.SINGLE,\n",
    "            Chem.rdchem.BondType.DOUBLE, \n",
    "            Chem.rdchem.BondType.TRIPLE,\n",
    "            Chem.rdchem.BondType.AROMATIC\n",
    "        ]\n",
    "        self.bonddir_list = [\n",
    "            Chem.rdchem.BondDir.NONE,\n",
    "            Chem.rdchem.BondDir.ENDUPRIGHT,\n",
    "            Chem.rdchem.BondDir.ENDDOWNRIGHT\n",
    "        ]\n",
    "\n",
    "    def calc_atom_features(self, atom: Chem.Atom) -> Tuple[list, list]:\n",
    "        \"\"\"Calculate atom features with better error handling\"\"\"\n",
    "        try:\n",
    "            # Basic features\n",
    "            atom_feat = [\n",
    "                self.atom_list.index(atom.GetAtomicNum()),\n",
    "                self.chirality_list.index(atom.GetChiralTag())\n",
    "            ]\n",
    "\n",
    "            # Physical features with error handling\n",
    "            phys_feat = []\n",
    "            \n",
    "            # Molecular weight contribution\n",
    "            try:\n",
    "                contrib_mw = Descriptors.ExactMolWt(Chem.MolFromSmiles(f'[{atom.GetSymbol()}]'))\n",
    "                phys_feat.append(contrib_mw)\n",
    "            except:\n",
    "                phys_feat.append(0.0)\n",
    "                \n",
    "            # LogP contribution    \n",
    "            try:\n",
    "                contrib_logp = Descriptors.MolLogP(Chem.MolFromSmiles(f'[{atom.GetSymbol()}]'))\n",
    "                phys_feat.append(contrib_logp)\n",
    "            except:\n",
    "                phys_feat.append(0.0)\n",
    "                \n",
    "            # Add other physical properties\n",
    "            phys_feat.extend([\n",
    "                atom.GetFormalCharge(),\n",
    "                int(atom.GetHybridization()),\n",
    "                int(atom.GetIsAromatic()),\n",
    "                atom.GetTotalNumHs(),\n",
    "                atom.GetTotalValence(),\n",
    "                atom.GetDegree()\n",
    "            ])\n",
    "            \n",
    "            return atom_feat, phys_feat\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating atom features: {e}\")\n",
    "            return [0, 0], [0.0] * 9\n",
    "\n",
    "    def get_atom_features(self, mol: Chem.Mol) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Extract atom features for the whole molecule\"\"\"\n",
    "        atom_feats = []\n",
    "        phys_feats = []\n",
    "        \n",
    "        if mol is None:\n",
    "            return torch.tensor([[0, 0]], dtype=torch.long), torch.tensor([[0.0] * 9], dtype=torch.float)\n",
    "            \n",
    "        for atom in mol.GetAtoms():\n",
    "            atom_feat, phys_feat = self.calc_atom_features(atom)\n",
    "            atom_feats.append(atom_feat)\n",
    "            phys_feats.append(phys_feat)\n",
    "\n",
    "        x = torch.tensor(atom_feats, dtype=torch.long)\n",
    "        phys = torch.tensor(phys_feats, dtype=torch.float)\n",
    "        \n",
    "        return x, phys\n",
    "    \n",
    "    def remove_unbonded_hydrogens(mol):\n",
    "        params = Chem.RemoveHsParameters()\n",
    "        params.removeDegreeZero = True\n",
    "        mol = Chem.RemoveHs(mol, params)\n",
    "        return mol\n",
    "\n",
    "\n",
    "    def get_bond_features(self, mol: Chem.Mol) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Extract bond features with better error handling\"\"\"\n",
    "        if mol is None:\n",
    "            return torch.tensor([[0], [0]], dtype=torch.long), torch.tensor([[0.0] * 5], dtype=torch.float)\n",
    "            \n",
    "        row, col, edge_feat = [], [], []\n",
    "        \n",
    "        for bond in mol.GetBonds():\n",
    "            try:\n",
    "                start, end = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
    "                \n",
    "                # Add edges in both directions\n",
    "                row += [start, end]\n",
    "                col += [end, start]\n",
    "                \n",
    "                # Bond features\n",
    "                bond_type = self.bond_list.index(bond.GetBondType())\n",
    "                bond_dir = self.bonddir_list.index(bond.GetBondDir())\n",
    "                \n",
    "                # Calculate additional properties\n",
    "                feat = [\n",
    "                    bond_type,\n",
    "                    bond_dir,\n",
    "                    int(bond.GetIsConjugated()),\n",
    "                    int(self._is_rotatable(bond)),\n",
    "                    self._get_bond_length(mol, start, end)\n",
    "                ]\n",
    "                \n",
    "                edge_feat.extend([feat, feat])\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing bond: {e}\")\n",
    "                continue\n",
    "\n",
    "        if not row:  # If no valid bonds were processed\n",
    "            return torch.tensor([[0], [0]], dtype=torch.long), torch.tensor([[0.0] * 5], dtype=torch.float)\n",
    "\n",
    "        edge_index = torch.tensor([row, col], dtype=torch.long)\n",
    "        edge_attr = torch.tensor(edge_feat, dtype=torch.float)\n",
    "        \n",
    "        return edge_index, edge_attr\n",
    "\n",
    "    def _is_rotatable(self, bond: Chem.Bond) -> bool:\n",
    "        \"\"\"Check if bond is rotatable\"\"\"\n",
    "        return (bond.GetBondType() == Chem.rdchem.BondType.SINGLE and \n",
    "                not bond.IsInRing() and\n",
    "                len(bond.GetBeginAtom().GetNeighbors()) > 1 and\n",
    "                len(bond.GetEndAtom().GetNeighbors()) > 1)\n",
    "\n",
    "    def _get_bond_length(self, mol: Chem.Mol, start: int, end: int) -> float:\n",
    "        \"\"\"Get bond length with error handling\"\"\"\n",
    "        try:\n",
    "            conf = mol.GetConformer()\n",
    "            if conf.Is3D():\n",
    "                return Chem.rdMolTransforms.GetBondLength(conf, start, end)\n",
    "        except:\n",
    "            pass\n",
    "        return 0.0\n",
    "\n",
    "    def process_molecule(self, smiles: str) -> Data:\n",
    "        \"\"\"Process SMILES string to graph data\"\"\"\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is None:\n",
    "                print(f\"Invalid SMILES: {smiles}\")\n",
    "                return None  # Skip invalid molecules\n",
    "            mol = RemoveHs(mol)\n",
    "\n",
    "            # Add explicit hydrogens\n",
    "            mol = Chem.AddHs(mol, addCoords=True)\n",
    "\n",
    "            # Sanitize molecule\n",
    "            Chem.SanitizeMol(mol)\n",
    "\n",
    "            # Check if the molecule has atoms\n",
    "            if mol.GetNumAtoms() == 0:\n",
    "                print(\"Molecule has no atoms, skipping.\")\n",
    "                return None\n",
    "\n",
    "            # Generate 3D coordinates\n",
    "            if not mol.GetNumConformers():\n",
    "                status = AllChem.EmbedMolecule(mol, AllChem.ETKDG())\n",
    "                if status != 0:\n",
    "                    print(\"Failed to generate 3D conformer\")\n",
    "                    return None  # Skip failed molecules\n",
    "\n",
    "                # Try MMFF or UFF optimization\n",
    "                try:\n",
    "                    AllChem.MMFFOptimizeMolecule(mol)\n",
    "                except:\n",
    "                    AllChem.UFFOptimizeMolecule(mol)\n",
    "\n",
    "            # Extract features\n",
    "            x_cat, x_phys = self.get_atom_features(mol)\n",
    "            edge_index, edge_attr = self.get_bond_features(mol)\n",
    "\n",
    "            # Create data object with SMILES\n",
    "            data = Data(\n",
    "                x_cat=x_cat, \n",
    "                x_phys=x_phys,\n",
    "                edge_index=edge_index, \n",
    "                edge_attr=edge_attr,\n",
    "                num_nodes=x_cat.size(0)\n",
    "            )\n",
    "\n",
    "            # Store the SMILES as an attribute\n",
    "            data.smiles = smiles\n",
    "\n",
    "            return data\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing molecule {smiles}: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35a311ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryQueue:\n",
    "    \"\"\"Memory queue with temporal decay for contrastive learning\"\"\"\n",
    "    def __init__(self, size: int, dim: int, decay: float = 0.99999):\n",
    "        self.size = size\n",
    "        self.dim = dim\n",
    "        self.decay = decay\n",
    "        self.ptr = 0\n",
    "        self.full = False\n",
    "        \n",
    "        # Initialize queue\n",
    "        self.queue = nn.Parameter(F.normalize(torch.randn(size, dim), dim=1), requires_grad=False)\n",
    "        self.queue_age = nn.Parameter(torch.zeros(size), requires_grad=False)\n",
    "        \n",
    "#         self.register_buffer(\"queue\", torch.randn(size, dim))\n",
    "#         self.register_buffer(\"queue_age\", torch.zeros(size))  # Track age of each entry\n",
    "        self.queue = F.normalize(self.queue, dim=1)\n",
    "        \n",
    "    def update_queue(self, keys: torch.Tensor):\n",
    "        \"\"\"Update queue with new keys\"\"\"\n",
    "        batch_size = keys.shape[0]\n",
    "        \n",
    "        # Increment age of all entries\n",
    "        self.queue_age += 1\n",
    "        \n",
    "        # Add new keys\n",
    "        if self.ptr + batch_size <= self.size:\n",
    "            self.queue[self.ptr:self.ptr + batch_size] = keys\n",
    "            self.queue_age[self.ptr:self.ptr + batch_size] = 0\n",
    "        else:\n",
    "            # Handle overflow\n",
    "            rem = self.size - self.ptr\n",
    "            self.queue[self.ptr:] = keys[:rem]\n",
    "            self.queue[:batch_size-rem] = keys[rem:]\n",
    "            self.queue_age[self.ptr:] = 0\n",
    "            self.queue_age[:batch_size-rem] = 0\n",
    "            self.full = True\n",
    "            \n",
    "        self.ptr = (self.ptr + batch_size) % self.size\n",
    "        \n",
    "    def get_decay_weights(self) -> torch.Tensor:\n",
    "        \"\"\"Get temporal decay weights for queue entries\"\"\"\n",
    "        return self.decay ** self.queue_age\n",
    "        \n",
    "    def compute_contrastive_loss(self, query: torch.Tensor, positive_key: torch.Tensor, \n",
    "                                temperature: float = 0.07) -> torch.Tensor:\n",
    "        \"\"\"Compute contrastive loss with temporal decay\"\"\"\n",
    "        # Normalize embeddings\n",
    "        query = F.normalize(query, dim=1)\n",
    "        positive_key = F.normalize(positive_key, dim=1)\n",
    "        queue = F.normalize(self.queue, dim=1)\n",
    "        \n",
    "        # Compute logits\n",
    "        l_pos = torch.einsum('nc,nc->n', [query, positive_key]).unsqueeze(-1)\n",
    "        l_neg = torch.einsum('nc,ck->nk', [query, queue.T])\n",
    "        \n",
    "        # Apply temporal decay to negative samples\n",
    "        decay_weights = self.get_decay_weights()\n",
    "        l_neg = l_neg * decay_weights.unsqueeze(0)\n",
    "        \n",
    "        # Temperature scaling\n",
    "        logits = torch.cat([l_pos, l_neg], dim=1) / temperature\n",
    "        labels = torch.zeros(logits.shape[0], dtype=torch.long, device=query.device)\n",
    "        \n",
    "        return F.cross_entropy(logits, labels)\n",
    "\n",
    "class GraphGenerator(nn.Module):\n",
    "    \"\"\"Generator network for molecular graph augmentations\"\"\"\n",
    "    def __init__(self, node_dim: int, edge_dim: int, hidden_dim: int = 128):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Node feature processing\n",
    "        self.node_encoder = nn.Sequential(\n",
    "            nn.Linear(node_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Edge feature processing\n",
    "        self.edge_encoder = nn.Sequential(\n",
    "            nn.Linear(edge_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Graph convolution layers\n",
    "        self.conv1 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GCNConv(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # Importance prediction layers\n",
    "        self.node_importance = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.edge_importance = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def normalize_features(self, x_cat, x_phys):\n",
    "        \"\"\"Normalize categorical and physical features separately\"\"\"\n",
    "        # Convert categorical features to one-hot\n",
    "        x_cat = x_cat.float()\n",
    "        \n",
    "        # Normalize physical features\n",
    "        x_phys = x_phys.float()\n",
    "        if x_phys.size(0) > 1:  # Only normalize if we have more than one sample\n",
    "            x_phys = (x_phys - x_phys.mean(0)) / (x_phys.std(0) + 1e-5)\n",
    "            \n",
    "        return x_cat, x_phys\n",
    "        \n",
    "    def forward(self, data) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # Normalize features\n",
    "        x_cat, x_phys = self.normalize_features(data.x_cat, data.x_phys)\n",
    "        \n",
    "        # Concatenate features\n",
    "        x = torch.cat([x_cat, x_phys], dim=-1)\n",
    "        \n",
    "        edge_index = data.edge_index\n",
    "        edge_attr = data.edge_attr.float()  # Ensure float type\n",
    "        \n",
    "        # Initial feature encoding\n",
    "        x = self.node_encoder(x)\n",
    "        edge_attr = self.edge_encoder(edge_attr)\n",
    "        \n",
    "        # Graph convolutions\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = self.conv3(x, edge_index)\n",
    "        \n",
    "        # Predict importance scores\n",
    "        node_scores = self.node_importance(x)\n",
    "        \n",
    "        # Edge scores using both connected nodes\n",
    "        edge_features = torch.cat([\n",
    "            x[edge_index[0]], \n",
    "            x[edge_index[1]]\n",
    "        ], dim=-1)\n",
    "        edge_scores = self.edge_importance(edge_features)\n",
    "        \n",
    "        return node_scores, edge_scores\n",
    "    \n",
    "def get_model_config(dataset):\n",
    "    \"\"\"Get model configuration based on dataset features\"\"\"\n",
    "    sample_data = dataset[0]\n",
    "    \n",
    "    # Calculate input dimensions\n",
    "    node_dim = sample_data.x_cat.shape[1] + sample_data.x_phys.shape[1]\n",
    "    edge_dim = sample_data.edge_attr.shape[1]\n",
    "    \n",
    "    config = GanClConfig(\n",
    "        node_dim=node_dim,\n",
    "        edge_dim=edge_dim,\n",
    "        hidden_dim=128,\n",
    "        output_dim=128,\n",
    "        temperature=0.07,\n",
    "        dropout_ratio=0.25\n",
    "    )\n",
    "    \n",
    "    return config\n",
    "\n",
    "class GraphDiscriminator(nn.Module):\n",
    "    \"\"\"Discriminator/Encoder network\"\"\"\n",
    "    def __init__(self, node_dim: int, edge_dim: int, hidden_dim: int = 128, output_dim: int = 128):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Feature encoding\n",
    "        self.node_encoder = nn.Sequential(\n",
    "            nn.Linear(node_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        self.edge_encoder = nn.Sequential(\n",
    "            nn.Linear(edge_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Graph convolution layers\n",
    "        self.conv1 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GCNConv(hidden_dim, output_dim)\n",
    "        \n",
    "        # Projection head for contrastive learning\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(output_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "        \n",
    "    def normalize_features(self, x_cat, x_phys):\n",
    "        \"\"\"Normalize categorical and physical features separately\"\"\"\n",
    "        # Convert categorical features to one-hot\n",
    "        x_cat = x_cat.float()\n",
    "        \n",
    "        # Normalize physical features\n",
    "        x_phys = x_phys.float()\n",
    "        if x_phys.size(0) > 1:  # Only normalize if we have more than one sample\n",
    "            x_phys = (x_phys - x_phys.mean(0)) / (x_phys.std(0) + 1e-5)\n",
    "            \n",
    "        return x_cat, x_phys \n",
    "        \n",
    "    def forward(self, data):\n",
    "        # Normalize features\n",
    "        x_cat, x_phys = self.normalize_features(data.x_cat, data.x_phys)\n",
    "        \n",
    "        # Concatenate features\n",
    "        x = torch.cat([x_cat, x_phys], dim=-1)\n",
    "        \n",
    "        edge_index = data.edge_index\n",
    "        edge_attr = data.edge_attr.float()  # Ensure float type\n",
    "        batch = data.batch\n",
    "        \n",
    "        # Initial feature encoding\n",
    "        x = self.node_encoder(x)\n",
    "        edge_attr = self.edge_encoder(edge_attr)\n",
    "        \n",
    "        # Graph convolutions\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = self.conv3(x, edge_index)\n",
    "        \n",
    "        # Global pooling\n",
    "        x = global_mean_pool(x, batch)\n",
    "        \n",
    "        # Projection\n",
    "        x = self.projection(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "@dataclass\n",
    "class GanClConfig:\n",
    "    \"\"\"Configuration for GAN-CL training\"\"\"\n",
    "    node_dim: int\n",
    "    edge_dim: int\n",
    "    hidden_dim: int = 128\n",
    "    output_dim: int = 128\n",
    "    temperature: float = 0.07\n",
    "    dropout_ratio: float = 0.25\n",
    "\n",
    "class MolecularGANCLBasic(nn.Module):\n",
    "    \"\"\"Basic GAN-CL framework without momentum encoder\"\"\"\n",
    "    def __init__(self, config: GanClConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Initialize networks with weight initialization\n",
    "        def init_weights(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                torch.nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.fill_(0.01)\n",
    "        \n",
    "        # Initialize Generator and Encoder\n",
    "        self.generator = GraphGenerator(\n",
    "            config.node_dim, \n",
    "            config.edge_dim, \n",
    "            config.hidden_dim * 2\n",
    "        )\n",
    "        self.generator.apply(init_weights)\n",
    "        \n",
    "        self.encoder = GraphDiscriminator(\n",
    "            config.node_dim,\n",
    "            config.edge_dim,\n",
    "            config.hidden_dim,\n",
    "            config.output_dim\n",
    "        )\n",
    "        self.encoder.apply(init_weights)\n",
    "        \n",
    "        # Loss weights\n",
    "        self.contrastive_weight = 1.0\n",
    "        self.adversarial_weight = 0.1\n",
    "        self.similarity_weight = 0.01\n",
    "        \n",
    "        # Temperature annealing for contrastive learning\n",
    "        self.initial_temperature = 0.1\n",
    "        self.min_temperature = 0.05\n",
    "        \n",
    "    def drop_graph_elements(self, data, node_scores: torch.Tensor, \n",
    "                          edge_scores: torch.Tensor) -> Data:\n",
    "        \"\"\"Apply dropout to graph based on importance scores\"\"\"\n",
    "        # Select elements to keep based on scores and dropout ratio\n",
    "        node_mask = (torch.rand_like(node_scores) > self.config.dropout_ratio).float()\n",
    "        edge_mask = (torch.rand_like(edge_scores) > self.config.dropout_ratio).float()\n",
    "        \n",
    "        # Apply masks\n",
    "        x_cat_new = data.x_cat * node_mask\n",
    "        x_phys_new = data.x_phys * node_mask\n",
    "        edge_attr_new = data.edge_attr * edge_mask\n",
    "        \n",
    "        # Create new graph data object\n",
    "        return Data(\n",
    "            x_cat=x_cat_new,\n",
    "            x_phys=x_phys_new,\n",
    "            edge_index=data.edge_index,\n",
    "            edge_attr=edge_attr_new,\n",
    "            batch=data.batch\n",
    "        )\n",
    "        \n",
    "    def get_temperature(self, epoch, total_epochs):\n",
    "        \"\"\"Anneal temperature during training\"\"\"\n",
    "        progress = epoch / total_epochs\n",
    "        return max(self.initial_temperature * (1 - progress), self.min_temperature)\n",
    "    \n",
    "    def compute_contrastive_loss(self, query_emb, key_emb, temperature=0.07):\n",
    "        \"\"\"Compute InfoNCE contrastive loss\"\"\"\n",
    "        # Normalize embeddings\n",
    "        query_emb = F.normalize(query_emb, dim=1)\n",
    "        key_emb = F.normalize(key_emb, dim=1)\n",
    "        \n",
    "        # Calculate similarity\n",
    "        logits = torch.mm(query_emb, key_emb.T) / temperature\n",
    "        \n",
    "        # Set labels to be the positive samples (diagonal elements)\n",
    "        labels = torch.arange(logits.shape[0], device=query_emb.device)\n",
    "        \n",
    "        return F.cross_entropy(logits, labels)\n",
    "    \n",
    "    def forward(self, data, epoch=0, total_epochs=50):\n",
    "        # Get current temperature\n",
    "        temperature = self.get_temperature(epoch, total_epochs)\n",
    "        \n",
    "        # Get importance scores from generator\n",
    "        node_scores, edge_scores = self.generator(data)\n",
    "        \n",
    "        # Create perturbed graph\n",
    "        perturbed_data = self.drop_graph_elements(data, node_scores, edge_scores)\n",
    "        \n",
    "        # Get embeddings\n",
    "        query_emb = self.encoder(perturbed_data)\n",
    "        original_emb = self.encoder(data)\n",
    "        \n",
    "        # Compute losses with modified weights\n",
    "        contrastive_loss = self.compute_contrastive_loss(query_emb, original_emb, temperature) * self.contrastive_weight\n",
    "        adversarial_loss = -F.mse_loss(query_emb, original_emb) * self.adversarial_weight\n",
    "        similarity_loss = F.mse_loss(query_emb, original_emb) * self.similarity_weight\n",
    "        \n",
    "        return contrastive_loss, adversarial_loss, similarity_loss\n",
    "    \n",
    "    def get_embeddings(self, data) -> torch.Tensor:\n",
    "        \"\"\"Get embeddings for downstream tasks\"\"\"\n",
    "        with torch.no_grad():\n",
    "            return self.encoder(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3eb075ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_molecule_metadata(dataset):\n",
    "    \"\"\"Extract metadata from PyG graph data without relying on SMILES strings\"\"\"\n",
    "    from rdkit import Chem\n",
    "    from rdkit.Chem import Descriptors\n",
    "    from tqdm import tqdm\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    import networkx as nx\n",
    "    from collections import defaultdict\n",
    "    \n",
    "    metadata = []\n",
    "    \n",
    "    for i, data in enumerate(tqdm(dataset, desc=\"Extracting molecule metadata\")):\n",
    "        # Set graph ID\n",
    "        mol_id = f\"molecule_{i}\"\n",
    "        \n",
    "        # Initialize empty dictionaries for metadata\n",
    "        properties = {}\n",
    "        features = {}\n",
    "        functional_groups = {}\n",
    "        ring_info = {\"ring_counts\": {}, \"ring_sizes\": {}}\n",
    "        \n",
    "        # Extract basic graph properties directly from the PyG data\n",
    "        if hasattr(data, 'num_nodes') and hasattr(data, 'edge_index'):\n",
    "            try:\n",
    "                # Convert to networkx graph for analysis\n",
    "                G = to_networkx(data)\n",
    "                \n",
    "                # Calculate graph-level properties\n",
    "                num_edges = data.edge_index.size(1) // 2  # Count unique edges\n",
    "                properties = {\n",
    "                    \"num_nodes\": data.num_nodes,\n",
    "                    \"num_edges\": num_edges,\n",
    "                    \"avg_node_degree\": 2 * num_edges / data.num_nodes if data.num_nodes > 0 else 0\n",
    "                }\n",
    "                \n",
    "                # Calculate average path length if graph is connected\n",
    "                if nx.is_connected(G):\n",
    "                    try:\n",
    "                        properties[\"avg_path_length\"] = nx.average_shortest_path_length(G)\n",
    "                    except:\n",
    "                        properties[\"avg_path_length\"] = 0.0\n",
    "                else:\n",
    "                    properties[\"avg_path_length\"] = 0.0\n",
    "                \n",
    "                # Add more sophisticated graph properties\n",
    "                try:\n",
    "                    properties[\"clustering_coefficient\"] = nx.average_clustering(G)\n",
    "                except:\n",
    "                    properties[\"clustering_coefficient\"] = 0.0\n",
    "                \n",
    "                try:\n",
    "                    properties[\"graph_diameter\"] = nx.diameter(G) if nx.is_connected(G) else 0\n",
    "                except:\n",
    "                    properties[\"graph_diameter\"] = 0\n",
    "\n",
    "                try:\n",
    "                    properties[\"assortativity\"] = nx.degree_assortativity_coefficient(G)\n",
    "                except:\n",
    "                    properties[\"assortativity\"] = 0.0\n",
    "                \n",
    "                # Graph features\n",
    "                features = {\n",
    "                    \"is_connected\": nx.is_connected(G),\n",
    "                    \"num_connected_components\": nx.number_connected_components(G),\n",
    "                    \"has_cycles\": not nx.is_tree(G),\n",
    "                    \"max_degree\": max(dict(G.degree()).values()) if G.number_of_nodes() > 0 else 0,\n",
    "                    \"density\": nx.density(G),\n",
    "                    \"is_bipartite\": nx.is_bipartite(G) if G.number_of_nodes() > 0 else False\n",
    "                }\n",
    "                \n",
    "                # Get centrality measures\n",
    "                if G.number_of_nodes() > 0:\n",
    "                    try:\n",
    "                        degree_centrality = nx.degree_centrality(G)\n",
    "                        features[\"max_centrality\"] = max(degree_centrality.values()) if degree_centrality else 0\n",
    "                        features[\"avg_centrality\"] = sum(degree_centrality.values()) / len(degree_centrality) if degree_centrality else 0\n",
    "                    except:\n",
    "                        features[\"max_centrality\"] = 0\n",
    "                        features[\"avg_centrality\"] = 0\n",
    "                else:\n",
    "                    features[\"max_centrality\"] = 0\n",
    "                    features[\"avg_centrality\"] = 0\n",
    "                \n",
    "                # Analyze node features if available\n",
    "                if hasattr(data, 'x_cat') and hasattr(data, 'x_phys'):\n",
    "                    # Atomic element distribution (from x_cat)\n",
    "                    atom_types = {}\n",
    "                    if data.x_cat.size(1) > 0:\n",
    "                        for i in range(data.num_nodes):\n",
    "                            atom_type = int(data.x_cat[i, 0].item())\n",
    "                            atom_types[atom_type] = atom_types.get(atom_type, 0) + 1\n",
    "                    \n",
    "                    features[\"atom_type_distribution\"] = atom_types\n",
    "                    \n",
    "                    # Physical property statistics (from x_phys)\n",
    "                    if data.x_phys.size(1) > 0:\n",
    "                        phys_means = data.x_phys.mean(dim=0).tolist() \n",
    "                        phys_stds = data.x_phys.std(dim=0).tolist()\n",
    "                        \n",
    "                        # Map indices to meaningful property names for the first few common properties\n",
    "                        phys_prop_names = ['contrib_mw', 'contrib_logp', 'formal_charge', \n",
    "                                        'hybridization', 'is_aromatic', 'num_h', 'valence', 'degree']\n",
    "                        \n",
    "                        for idx, name in enumerate(phys_prop_names):\n",
    "                            if idx < len(phys_means):\n",
    "                                properties[f\"avg_{name}\"] = phys_means[idx]\n",
    "                                properties[f\"std_{name}\"] = phys_stds[idx]\n",
    "                \n",
    "                # Cycle analysis\n",
    "                cycles = list(nx.cycle_basis(G))\n",
    "                cycle_count = len(cycles)\n",
    "                ring_info[\"ring_counts\"][\"total\"] = cycle_count\n",
    "                \n",
    "                # Count rings by size\n",
    "                ring_sizes = defaultdict(int)\n",
    "                for cycle in cycles:\n",
    "                    size = len(cycle)\n",
    "                    ring_sizes[str(size)] = ring_sizes.get(str(size), 0) + 1\n",
    "                \n",
    "                # Ensure we have entries for common ring sizes\n",
    "                for size in range(3, 11):\n",
    "                    if str(size) not in ring_sizes:\n",
    "                        ring_sizes[str(size)] = 0\n",
    "                \n",
    "                ring_info[\"ring_sizes\"] = dict(ring_sizes)\n",
    "                \n",
    "                # Estimate ring types\n",
    "                ring_info[\"ring_counts\"][\"single\"] = 0\n",
    "                ring_info[\"ring_counts\"][\"fused\"] = 0\n",
    "                \n",
    "                # Identify single vs fused rings by checking for shared nodes\n",
    "                if cycles:\n",
    "                    # Build a mapping of nodes to cycles they belong to\n",
    "                    node_to_cycles = defaultdict(list)\n",
    "                    for cycle_idx, cycle in enumerate(cycles):\n",
    "                        for node in cycle:\n",
    "                            node_to_cycles[node].append(cycle_idx)\n",
    "                    \n",
    "                    # Count single rings (no shared nodes with other rings)\n",
    "                    shared_cycles = set()\n",
    "                    for node, cycle_list in node_to_cycles.items():\n",
    "                        if len(cycle_list) > 1:\n",
    "                            for c in cycle_list:\n",
    "                                shared_cycles.add(c)\n",
    "                    \n",
    "                    ring_info[\"ring_counts\"][\"single\"] = cycle_count - len(shared_cycles)\n",
    "                    ring_info[\"ring_counts\"][\"fused\"] = len(shared_cycles)\n",
    "                \n",
    "                # Edge feature analysis if available\n",
    "                if hasattr(data, 'edge_attr') and data.edge_attr.size(0) > 0:\n",
    "                    # Analyze bond types (assuming first dimension is bond type)\n",
    "                    bond_types = {}\n",
    "                    for i in range(data.edge_attr.size(0)):\n",
    "                        if data.edge_attr.size(1) > 0:\n",
    "                            bond_type = int(data.edge_attr[i, 0].item())\n",
    "                            bond_types[bond_type] = bond_types.get(bond_type, 0) + 1\n",
    "                    \n",
    "                    # Divide by 2 since each bond is counted twice in undirected graph\n",
    "                    for bt in bond_types:\n",
    "                        bond_types[bt] = bond_types[bt] // 2\n",
    "                    \n",
    "                    functional_groups[\"bond_types\"] = bond_types\n",
    "                    \n",
    "                    # Count functional group proxies based on patterns in the graph\n",
    "                    # This is just an estimate since we don't have chemical information\n",
    "                    conjugated_bonds = 0\n",
    "                    for i in range(data.edge_attr.size(0)):\n",
    "                        if data.edge_attr.size(1) > 1 and data.edge_attr[i, 2].item() > 0:  # IsConjugated flag\n",
    "                            conjugated_bonds += 1\n",
    "                    \n",
    "                    functional_groups[\"conjugated_bonds\"] = conjugated_bonds // 2\n",
    "                    \n",
    "            except Exception as e:\n",
    "                # If any error occurs during analysis, use minimal information\n",
    "                print(f\"Error analyzing graph {i}: {e}\")\n",
    "        \n",
    "        metadata.append({\n",
    "            \"graph_id\": mol_id,\n",
    "            \"properties\": properties,\n",
    "            \"features\": features,\n",
    "            \"functional_groups\": functional_groups,\n",
    "            \"ring_info\": ring_info\n",
    "        })\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "def to_networkx(data):\n",
    "    \"\"\"Convert PyG data to networkx graph for analysis\"\"\"\n",
    "    import networkx as nx\n",
    "    \n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add nodes\n",
    "    for i in range(data.num_nodes):\n",
    "        G.add_node(i)\n",
    "    \n",
    "    # Add edges (removing duplicates and self-loops)\n",
    "    edge_index = data.edge_index.cpu().numpy()\n",
    "    edges = set()\n",
    "    for i in range(edge_index.shape[1]):\n",
    "        u, v = edge_index[0, i], edge_index[1, i]\n",
    "        if u != v and (u, v) not in edges and (v, u) not in edges:\n",
    "            G.add_edge(u, v)\n",
    "            edges.add((u, v))\n",
    "    \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e90813e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embedding_file(embeddings, molecule_indices, training_info, model_config, filepath):\n",
    "    \"\"\"Save embeddings with training metadata\"\"\"\n",
    "    data = {\n",
    "        \"embeddings\": embeddings,\n",
    "        \"molecule_indices\": molecule_indices,\n",
    "        \"training_info\": training_info,\n",
    "        \"model_config\": {k: v for k, v in model_config.__dict__.items() \n",
    "                         if not k.startswith('_') and not callable(v)}\n",
    "    }\n",
    "    \n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "        \n",
    "\n",
    "def extract_negative_pair_distances_basic(model, train_loader, device, epoch_num, num_samples=1000):\n",
    "    \"\"\"Extract distances between negative pairs at a specific epoch (for basic model without momentum)\"\"\"\n",
    "    model.eval()\n",
    "    negative_distances = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(train_loader, desc=f\"Extracting negative pairs at epoch {epoch_num}\"):\n",
    "            batch = batch.to(device)\n",
    "            \n",
    "            # Get original embeddings\n",
    "            original_emb = model.encoder(batch)\n",
    "            \n",
    "            # Get augmented version\n",
    "            node_scores, edge_scores = model.generator(batch)\n",
    "            perturbed_data = model.drop_graph_elements(batch, node_scores, edge_scores)\n",
    "            perturbed_emb = model.encoder(perturbed_data)\n",
    "            \n",
    "            # Normalize embeddings\n",
    "            original_emb = F.normalize(original_emb, dim=1)\n",
    "            perturbed_emb = F.normalize(perturbed_emb, dim=1)\n",
    "            \n",
    "            # Compute distances between all pairs\n",
    "            similarity_matrix = torch.mm(original_emb, perturbed_emb.T)\n",
    "            \n",
    "            # Get off-diagonal elements (negative pairs)\n",
    "            mask = ~torch.eye(similarity_matrix.shape[0], dtype=torch.bool, device=device)\n",
    "            negative_similarities = similarity_matrix[mask]\n",
    "            \n",
    "            # Convert to distances (1 - similarity)\n",
    "            distances = 1 - negative_similarities\n",
    "            \n",
    "            negative_distances.append(distances.cpu().numpy())\n",
    "            \n",
    "            # If we have enough samples, break\n",
    "            if len(negative_distances) * batch.batch[-1].item() > num_samples:\n",
    "                break\n",
    "    \n",
    "    # Concatenate and sample if necessary\n",
    "    negative_distances = np.concatenate(negative_distances)\n",
    "    if len(negative_distances) > num_samples:\n",
    "        indices = np.random.choice(len(negative_distances), num_samples, replace=False)\n",
    "        negative_distances = negative_distances[indices]\n",
    "    \n",
    "    return negative_distances        \n",
    "        \n",
    "def save_embeddings_with_molecules(embeddings, dataset, filepath):\n",
    "    \"\"\"Save embeddings with corresponding molecule information and graph-level properties\"\"\"\n",
    "    # Create a list to store molecule data\n",
    "    molecule_data = []\n",
    "    \n",
    "    # Extract important info from each molecule in the dataset\n",
    "    for data in dataset:\n",
    "        # Create a dictionary with basic graph properties\n",
    "        mol_info = {\n",
    "            \"num_nodes\": data.num_nodes,\n",
    "            \"edge_index\": data.edge_index.tolist() if hasattr(data, 'edge_index') else None,\n",
    "            \"x_cat\": data.x_cat.tolist() if hasattr(data, 'x_cat') else None,\n",
    "            \"x_phys\": data.x_phys.tolist() if hasattr(data, 'x_phys') else None,\n",
    "            \"edge_attr\": data.edge_attr.tolist() if hasattr(data, 'edge_attr') else None,\n",
    "            \"smiles\": data.smiles if hasattr(data, 'smiles') else None\n",
    "        }\n",
    "        \n",
    "        # Calculate additional graph properties if possible\n",
    "        try:\n",
    "            if hasattr(data, 'edge_index') and hasattr(data, 'num_nodes'):\n",
    "                # Graph density\n",
    "                num_edges = len(data.edge_index[0]) // 2  # Undirected edges counted once\n",
    "                max_edges = data.num_nodes * (data.num_nodes - 1) // 2\n",
    "                density = num_edges / max_edges if max_edges > 0 else 0\n",
    "                mol_info[\"graph_density\"] = density\n",
    "                \n",
    "                # Average degree\n",
    "                avg_degree = num_edges * 2 / data.num_nodes if data.num_nodes > 0 else 0\n",
    "                mol_info[\"avg_degree\"] = avg_degree\n",
    "                \n",
    "                # Count atom types if available\n",
    "                if hasattr(data, 'x_cat') and data.x_cat is not None:\n",
    "                    atom_types = {}\n",
    "                    for atom in data.x_cat:\n",
    "                        atom_type = int(atom[0])\n",
    "                        atom_types[atom_type] = atom_types.get(atom_type, 0) + 1\n",
    "                    mol_info[\"atom_type_counts\"] = atom_types\n",
    "                \n",
    "                # Count bond types if available\n",
    "                if hasattr(data, 'edge_attr') and data.edge_attr is not None:\n",
    "                    bond_types = {}\n",
    "                    for bond in data.edge_attr:\n",
    "                        bond_type = int(bond[0])\n",
    "                        bond_types[bond_type] = bond_types.get(bond_type, 0) + 1\n",
    "                    mol_info[\"bond_type_counts\"] = bond_types\n",
    "        except:\n",
    "            # If calculation fails, continue without these properties\n",
    "            pass\n",
    "            \n",
    "        molecule_data.append(mol_info)\n",
    "    \n",
    "    # Save both embeddings and molecule data\n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'embeddings': embeddings,\n",
    "            'molecule_data': molecule_data,\n",
    "            'graph_properties': True,\n",
    "            'smiles_list': [data.smiles for data in dataset if hasattr(data, 'smiles')]\n",
    "        }, f)\n",
    "    \n",
    "    print(f\"Saved embeddings and molecule data with graph properties to {filepath}\")\n",
    "        \n",
    "def extract_embeddings_at_epoch(model, train_loader, device, epoch_num):\n",
    "    batch_size = 32\n",
    "    \"\"\"Extract embeddings at a specific epoch for visualization/analysis\"\"\"\n",
    "    model.eval()\n",
    "    epoch_embeddings = []\n",
    "    molecule_indices = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tqdm(train_loader, desc=f\"Extracting embeddings at epoch {epoch_num}\")):\n",
    "            batch = batch.to(device)\n",
    "            embeddings = model.get_embeddings(batch)\n",
    "            epoch_embeddings.append(embeddings.cpu().numpy())\n",
    "            \n",
    "            # Track molecule indices \n",
    "            if hasattr(batch, 'batch'):\n",
    "                indices = [i * batch_size + j for j in range(batch.batch[-1].item() + 1)]\n",
    "                molecule_indices.extend(indices)\n",
    "            else:\n",
    "                molecule_indices.append(i)\n",
    "    \n",
    "    # Concatenate all embeddings\n",
    "    epoch_embeddings = np.vstack(epoch_embeddings)\n",
    "    \n",
    "    return epoch_embeddings, molecule_indices\n",
    "        \n",
    "def save_embeddings(embeddings, labels, filepath):\n",
    "    \"\"\"Save embeddings and corresponding labels\"\"\"\n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'embeddings': embeddings,\n",
    "            'labels': labels\n",
    "        }, f)\n",
    "\n",
    "def save_encoder(encoder, save_path, info=None):\n",
    "    \"\"\"Save encoder model for downstream tasks\"\"\"\n",
    "    save_dict = {\n",
    "        'encoder_state_dict': encoder.state_dict(),\n",
    "        'model_info': info or {}\n",
    "    }\n",
    "    torch.save(save_dict, save_path)\n",
    "\n",
    "def load_encoder(model_path, device='cpu'):\n",
    "    \"\"\"Load saved encoder model\"\"\"\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    encoder = GraphDiscriminator(\n",
    "        node_dim=checkpoint['model_info'].get('node_dim'),\n",
    "        edge_dim=checkpoint['model_info'].get('edge_dim'),\n",
    "        hidden_dim=checkpoint['model_info'].get('hidden_dim', 128),\n",
    "        output_dim=checkpoint['model_info'].get('output_dim', 128)\n",
    "    )\n",
    "    encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "    return encoder        \n",
    "        \n",
    "def train_gan_cl_basic(train_loader, config, dataset, device='cuda', \n",
    "                      save_dir='./checkpoints_basic', \n",
    "                      embedding_dir='./embeddings_basic'):\n",
    "    \"\"\"Training function for GAN-CL without momentum encoder\"\"\"\n",
    "    \n",
    "    # Create directories\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    os.makedirs(embedding_dir, exist_ok=True)\n",
    "    metadata_dir = os.path.join(embedding_dir, 'metadata')\n",
    "    os.makedirs(metadata_dir, exist_ok=True)\n",
    "    \n",
    "    # Timestamp for file naming\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    # Save molecule indices for consistent order\n",
    "    molecule_indices = list(range(len(dataset)))\n",
    "    \n",
    "    # Initialize model\n",
    "    model = MolecularGANCLBasic(config).to(device)\n",
    "    \n",
    "    # Extract pre-training embeddings before any training\n",
    "    print(\"Extracting pre-training embeddings...\")\n",
    "    model.eval()\n",
    "    pre_training_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(train_loader, desc=\"Pre-training embeddings\"):\n",
    "            batch = batch.to(device)\n",
    "            embeddings = model.get_embeddings(batch)\n",
    "            pre_training_embeddings.append(embeddings.cpu())\n",
    "    \n",
    "    pre_training_embeddings = torch.cat(pre_training_embeddings, dim=0).numpy()\n",
    "    \n",
    "    # Save pre-training embeddings\n",
    "    pre_training_info = {\n",
    "        \"stage\": \"pre\",\n",
    "        \"epoch\": 0,\n",
    "        \"timestamp\": timestamp,\n",
    "        \"loss_values\": {\"contrastive\": 0, \"adversarial\": 0, \"similarity\": 0, \"total\": 0}\n",
    "    }\n",
    "    \n",
    "    save_embedding_file(\n",
    "        pre_training_embeddings, \n",
    "        molecule_indices,\n",
    "        pre_training_info, \n",
    "        config, \n",
    "        os.path.join(embedding_dir, f'pre_training_embeddings_{timestamp}.pkl')\n",
    "    )\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    # Initialize optimizers\n",
    "    optimizer_encoder = torch.optim.Adam(model.encoder.parameters(), lr=3e-4)\n",
    "    optimizer_generator = torch.optim.Adam(model.generator.parameters(), lr=1e-4)\n",
    "    \n",
    "    # Training metrics\n",
    "    metrics = {\n",
    "        'contrastive_losses': [],\n",
    "        'adversarial_losses': [],\n",
    "        'similarity_losses': [],\n",
    "        'total_losses': []\n",
    "    }\n",
    "    \n",
    "    # Training phases\n",
    "    print(\"Phase 1: Pretraining Contrastive Learning...\")\n",
    "    pretrain_epochs = 10\n",
    "    for epoch in range(pretrain_epochs):\n",
    "        contrastive_epoch_loss = 0\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f'Pretrain Epoch {epoch+1}/{pretrain_epochs}'):\n",
    "            batch = batch.to(device)\n",
    "            \n",
    "            # Forward pass (without generator)\n",
    "            query_emb = model.encoder(batch)\n",
    "            key_emb = model.encoder(batch) # No momentum encoder - using same encoder with different augmentation\n",
    "            \n",
    "            # Compute contrastive loss\n",
    "            contrastive_loss = model.compute_contrastive_loss(\n",
    "                query_emb, key_emb, model.config.temperature\n",
    "            )\n",
    "            \n",
    "            # Update encoder\n",
    "            optimizer_encoder.zero_grad()\n",
    "            contrastive_loss.backward()\n",
    "            optimizer_encoder.step()\n",
    "            \n",
    "            contrastive_epoch_loss += contrastive_loss.item()\n",
    "            \n",
    "        avg_loss = contrastive_epoch_loss / len(train_loader)\n",
    "        metrics['contrastive_losses'].append(avg_loss)\n",
    "        print(f'Pretrain Epoch {epoch+1}, Avg Loss: {avg_loss:.4f}')\n",
    "        \n",
    "        # Save embeddings at key epochs for temporal analysis\n",
    "        if (epoch + 1) % 2 == 0 or epoch == 0:\n",
    "            epoch_embeddings, _ = extract_embeddings_at_epoch(model, train_loader, device, epoch+1)\n",
    "            \n",
    "            # Save epoch embeddings\n",
    "            epoch_info = {\n",
    "                \"stage\": f\"pretrain_epoch_{epoch+1}\",\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"timestamp\": datetime.now().strftime('%Y%m%d_%H%M%S'),\n",
    "                \"loss_values\": {\"contrastive\": avg_loss}\n",
    "            }\n",
    "            \n",
    "            save_embedding_file(\n",
    "                epoch_embeddings,\n",
    "                molecule_indices,\n",
    "                epoch_info,\n",
    "                config,\n",
    "                os.path.join(embedding_dir, f'pretrain_epoch_{epoch+1}_embeddings_{timestamp}.pkl')\n",
    "            )\n",
    "        \n",
    "            negative_distances = extract_negative_pair_distances_basic(model, train_loader, device, epoch+1)\n",
    "    \n",
    "            # Save to file\n",
    "            negative_pair_file = os.path.join(embedding_dir, f'basic_pretrain_negative_pairs_epoch_{epoch+1}_{timestamp}.pkl')\n",
    "            with open(negative_pair_file, 'wb') as f:\n",
    "                pickle.dump({\n",
    "                    'epoch': epoch + 1,\n",
    "                    'negative_distances': negative_distances,\n",
    "                    'timestamp': datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "                }, f)\n",
    "\n",
    "            print(f\"Saved negative pair analysis data to {negative_pair_file}\")\n",
    "        \n",
    "        # Save pretrained checkpoint\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_encoder_state_dict': optimizer_encoder.state_dict(),\n",
    "                'loss': avg_loss,\n",
    "            }, os.path.join(save_dir, f'pretrain_checkpoint_{epoch+1}.pt'))\n",
    "    \n",
    "    print(\"\\nPhase 2: Training GAN-CL...\")\n",
    "    train_epochs = 50\n",
    "    for epoch in range(train_epochs):\n",
    "        epoch_losses = {\n",
    "            'contrastive': 0,\n",
    "            'adversarial': 0,\n",
    "            'similarity': 0,\n",
    "            'total': 0\n",
    "        }\n",
    "\n",
    "        for batch in tqdm(train_loader, desc=f'Train Epoch {epoch+1}/{train_epochs}'):\n",
    "            batch = batch.to(device)\n",
    "\n",
    "            # Step 1: Train Encoder\n",
    "            optimizer_encoder.zero_grad()\n",
    "\n",
    "            # Get importance scores from generator\n",
    "            with torch.no_grad():\n",
    "                node_scores, edge_scores = model.generator(batch)\n",
    "\n",
    "            # Create perturbed graph\n",
    "            perturbed_data = model.drop_graph_elements(batch, node_scores, edge_scores)\n",
    "\n",
    "            # Get embeddings\n",
    "            query_emb = model.encoder(perturbed_data)\n",
    "            key_emb = model.encoder(batch)  # No momentum encoder\n",
    "\n",
    "            # Compute losses for encoder\n",
    "            contrastive_loss = model.compute_contrastive_loss(\n",
    "                query_emb, key_emb, model.config.temperature\n",
    "            )\n",
    "            similarity_loss = F.mse_loss(query_emb, key_emb)\n",
    "\n",
    "            # Total loss for encoder\n",
    "            encoder_loss = contrastive_loss + 0.1 * similarity_loss\n",
    "\n",
    "            # Update encoder\n",
    "            encoder_loss.backward()\n",
    "            optimizer_encoder.step()\n",
    "\n",
    "            # Step 2: Train Generator\n",
    "            optimizer_generator.zero_grad()\n",
    "\n",
    "            # Get new embeddings for adversarial loss\n",
    "            node_scores, edge_scores = model.generator(batch)\n",
    "            perturbed_data = model.drop_graph_elements(batch, node_scores, edge_scores)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                original_emb = model.encoder(batch)\n",
    "            perturbed_emb = model.encoder(perturbed_data)\n",
    "\n",
    "            # Compute adversarial loss\n",
    "            adversarial_loss = -F.mse_loss(perturbed_emb, original_emb)\n",
    "\n",
    "            # Update generator\n",
    "            adversarial_loss.backward()\n",
    "            optimizer_generator.step()\n",
    "\n",
    "            # Update metrics\n",
    "            epoch_losses['contrastive'] += contrastive_loss.item()\n",
    "            epoch_losses['adversarial'] += adversarial_loss.item()\n",
    "            epoch_losses['similarity'] += similarity_loss.item()\n",
    "            epoch_losses['total'] += encoder_loss.item()\n",
    "\n",
    "        # Average losses\n",
    "        for k in epoch_losses:\n",
    "            epoch_losses[k] /= len(train_loader)\n",
    "            metrics[f'{k}_losses'].append(epoch_losses[k])\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Losses: {epoch_losses}')\n",
    "\n",
    "        # Extract and save embeddings periodically\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            model.eval()\n",
    "            checkpoint_embeddings = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch in train_loader:\n",
    "                    batch = batch.to(device)\n",
    "                    embeddings = model.get_embeddings(batch)\n",
    "                    checkpoint_embeddings.append(embeddings.cpu())\n",
    "            \n",
    "            checkpoint_embeddings = torch.cat(checkpoint_embeddings, dim=0).numpy()\n",
    "            \n",
    "            # Save checkpoint embeddings with training info\n",
    "            checkpoint_info = {\n",
    "                \"stage\": f\"epoch_{epoch+1}\",\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"timestamp\": datetime.now().strftime('%Y%m%d_%H%M%S'),\n",
    "                \"loss_values\": epoch_losses\n",
    "            }\n",
    "            \n",
    "            save_embedding_file(\n",
    "                checkpoint_embeddings,\n",
    "                molecule_indices,\n",
    "                checkpoint_info,\n",
    "                config,\n",
    "                os.path.join(embedding_dir, f'epoch_{epoch+1}_embeddings_{timestamp}.pkl')\n",
    "            )\n",
    "            \n",
    "            negative_distances = extract_negative_pair_distances_basic(model, train_loader, device, epoch+1)\n",
    "    \n",
    "            # Save to file\n",
    "            negative_pair_file = os.path.join(embedding_dir, f'basic_negative_pairs_epoch_{epoch+1}_{timestamp}.pkl')\n",
    "            with open(negative_pair_file, 'wb') as f:\n",
    "                pickle.dump({\n",
    "                    'epoch': epoch + 1,\n",
    "                    'negative_distances': negative_distances,\n",
    "                    'timestamp': datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "                }, f)\n",
    "\n",
    "            print(f\"Saved negative pair analysis data to {negative_pair_file}\")\n",
    "            \n",
    "            model.train()\n",
    "\n",
    "        # Save checkpoint\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_encoder_state_dict': optimizer_encoder.state_dict(),\n",
    "                'optimizer_generator_state_dict': optimizer_generator.state_dict(),\n",
    "                'losses': epoch_losses,\n",
    "            }, os.path.join(save_dir, f'gan_cl_checkpoint_{epoch+1}.pt'))\n",
    "    \n",
    "    # Extract and save post-training embeddings at the end\n",
    "    print(\"Extracting post-training embeddings...\")\n",
    "    model.eval()\n",
    "    post_training_embeddings = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(train_loader, desc=\"Post-training embeddings\"):\n",
    "            batch = batch.to(device)\n",
    "            embeddings = model.get_embeddings(batch)\n",
    "            post_training_embeddings.append(embeddings.cpu())\n",
    "        \n",
    "    post_training_embeddings = torch.cat(post_training_embeddings, dim=0).numpy()\n",
    "    \n",
    "    # Save post-training embeddings\n",
    "    post_training_info = {\n",
    "        \"stage\": \"post\",\n",
    "        \"epoch\": train_epochs,\n",
    "        \"timestamp\": datetime.now().strftime('%Y%m%d_%H%M%S'),\n",
    "        \"loss_values\": epoch_losses\n",
    "    }\n",
    "    \n",
    "    save_embedding_file(\n",
    "        post_training_embeddings,\n",
    "        molecule_indices,\n",
    "        post_training_info,\n",
    "        config,\n",
    "        os.path.join(embedding_dir, f'post_training_embeddings_{timestamp}.pkl')\n",
    "    )\n",
    "    \n",
    "    # Save final embeddings with molecule data\n",
    "    final_embedding_path = os.path.join(embedding_dir, f'final_embeddings_molecules_{timestamp}.pkl')\n",
    "    save_embeddings_with_molecules(post_training_embeddings, dataset, final_embedding_path)\n",
    "    \n",
    "    return model, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dbc0517",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data loading...\n",
      "Failed to generate 3D conformer\n",
      "Failed to generate 3D conformer\n",
      "Failed to generate 3D conformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:16:36] UFFTYPER: Unrecognized atom type: Se2+2 (17)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to generate 3D conformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:16:40] UFFTYPER: Unrecognized charge state for atom: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to generate 3D conformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:16:48] UFFTYPER: Unrecognized charge state for atom: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to generate 3D conformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:17:18] UFFTYPER: Unrecognized atom type: S_5+4 (11)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to generate 3D conformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:17:38] UFFTYPER: Unrecognized atom type: Se2+2 (14)\n",
      "[19:17:38] UFFTYPER: Unrecognized charge state for atom: 20\n",
      "[19:17:38] UFFTYPER: Unrecognized charge state for atom: 40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to generate 3D conformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:17:39] UFFTYPER: Unrecognized charge state for atom: 5\n",
      "[19:17:43] UFFTYPER: Unrecognized charge state for atom: 9\n",
      "[19:17:48] UFFTYPER: Unrecognized charge state for atom: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to generate 3D conformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:18:01] UFFTYPER: Unrecognized charge state for atom: 8\n",
      "[19:18:39] UFFTYPER: Unrecognized charge state for atom: 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to generate 3D conformer\n",
      "Failed to generate 3D conformer\n",
      "Failed to generate 3D conformer\n",
      "Failed to generate 3D conformer\n",
      "Failed to generate 3D conformer\n",
      "Failed to generate 3D conformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:20:22] UFFTYPER: Unrecognized charge state for atom: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to generate 3D conformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:20:39] UFFTYPER: Unrecognized charge state for atom: 3\n",
      "[19:20:39] UFFTYPER: Unrecognized charge state for atom: 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to generate 3D conformer\n",
      "Failed to generate 3D conformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:21:35] UFFTYPER: Unrecognized atom type: S_6+6 (17)\n",
      "[19:21:44] UFFTYPER: Unrecognized atom type: Se2+2 (7)\n",
      "[19:21:44] UFFTYPER: Unrecognized atom type: Se2+2 (7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to generate 3D conformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:22:01] UFFTYPER: Unrecognized charge state for atom: 5\n",
      "[19:22:16] UFFTYPER: Unrecognized charge state for atom: 13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to generate 3D conformer\n",
      "Failed to generate 3D conformer\n",
      "Failed to generate 3D conformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:22:59] UFFTYPER: Unrecognized charge state for atom: 8\n",
      "[19:23:12] UFFTYPER: Unrecognized atom type: Se2+2 (16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to generate 3D conformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:23:32] UFFTYPER: Unrecognized charge state for atom: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to generate 3D conformer\n",
      "Failed to generate 3D conformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:24:07] UFFTYPER: Unrecognized charge state for atom: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to generate 3D conformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:25:26] UFFTYPER: Unrecognized charge state for atom: 17\n",
      "[19:25:26] UFFTYPER: Unrecognized charge state for atom: 19\n",
      "[19:25:29] UFFTYPER: Unrecognized charge state for atom: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to generate 3D conformer\n",
      "Failed to generate 3D conformer\n",
      "Failed to generate 3D conformer\n",
      "Failed to generate 3D conformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:27:44] UFFTYPER: Unrecognized charge state for atom: 8\n",
      "[19:27:46] UFFTYPER: Unrecognized charge state for atom: 1\n",
      "[19:28:02] UFFTYPER: Unrecognized atom type: Se2+2 (9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to generate 3D conformer\n",
      "Failed to generate 3D conformer\n",
      "Failed to generate 3D conformer\n",
      "Failed to generate 3D conformer\n",
      "Failed to generate 3D conformer\n",
      "Failed to generate 3D conformer\n",
      "Failed to generate 3D conformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:29:08] UFFTYPER: Unrecognized atom type: S_5+4 (1)\n",
      "[19:29:18] UFFTYPER: Unrecognized atom type: S_5+4 (10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to generate 3D conformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:29:37] UFFTYPER: Unrecognized hybridization for atom: 1\n",
      "[19:29:37] UFFTYPER: Unrecognized atom type: Pt+2 (1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to generate 3D conformer\n",
      "Failed to generate 3D conformer\n",
      "Failed to generate 3D conformer\n",
      "Failed to generate 3D conformer\n",
      "Failed to generate 3D conformer\n",
      "Failed to generate 3D conformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:33:20] UFFTYPER: Unrecognized charge state for atom: 3\n",
      "[19:33:21] UFFTYPER: Unrecognized charge state for atom: 15\n",
      "[19:33:26] UFFTYPER: Unrecognized atom type: S_5+4 (15)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to generate 3D conformer\n",
      "Failed to generate 3D conformer\n",
      "Failed to generate 3D conformer\n",
      "Failed to generate 3D conformer\n",
      "Failed to generate 3D conformer\n",
      "Failed to generate 3D conformer\n",
      "Failed to generate 3D conformer\n",
      "Failed to generate 3D conformer\n",
      "Failed to generate 3D conformer\n",
      "Failed to generate 3D conformer\n",
      "Failed to generate 3D conformer\n",
      "Failed to generate 3D conformer\n",
      "Failed to generate 3D conformer\n",
      "Failed to generate 3D conformer\n",
      "Failed to generate 3D conformer\n",
      "Failed to generate 3D conformer\n",
      "Failed to generate 3D conformer\n",
      "Failed to generate 3D conformer\n",
      "Failed to generate 3D conformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Malli\\anaconda3\\envs\\baceenv\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loaded dataset with 9937 graphs.\n",
      "2. Failed SMILES count: 63\n",
      "3. Created DataLoader with 9937 graphs\n",
      "4. Using device: cpu\n",
      "5. Starting basic GAN-CL training (without momentum encoder)...\n",
      "Extracting pre-training embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pre-training embeddings: 100%|| 311/311 [00:03<00:00, 99.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 1: Pretraining Contrastive Learning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 1/10: 100%|| 311/311 [00:45<00:00,  6.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 1, Avg Loss: 0.1791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings at epoch 1: 100%|| 311/311 [00:03<00:00, 97.39it/s]\n",
      "Extracting negative pairs at epoch 1:  10%|                                      | 32/311 [00:01<00:11, 24.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved negative pair analysis data to ./embeddings_basic\\basic_pretrain_negative_pairs_epoch_1_20250331_193728.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 2/10: 100%|| 311/311 [01:00<00:00,  5.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 2, Avg Loss: 0.0132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings at epoch 2: 100%|| 311/311 [00:05<00:00, 61.22it/s]\n",
      "Extracting negative pairs at epoch 2:  10%|                                      | 32/311 [00:02<00:17, 15.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved negative pair analysis data to ./embeddings_basic\\basic_pretrain_negative_pairs_epoch_2_20250331_193728.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 3/10: 100%|| 311/311 [00:56<00:00,  5.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 3, Avg Loss: 0.0092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 4/10: 100%|| 311/311 [00:50<00:00,  6.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 4, Avg Loss: 0.0072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings at epoch 4: 100%|| 311/311 [00:03<00:00, 89.25it/s]\n",
      "Extracting negative pairs at epoch 4:  10%|                                      | 32/311 [00:01<00:12, 22.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved negative pair analysis data to ./embeddings_basic\\basic_pretrain_negative_pairs_epoch_4_20250331_193728.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 5/10: 100%|| 311/311 [00:59<00:00,  5.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 5, Avg Loss: 0.0061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 6/10: 100%|| 311/311 [00:56<00:00,  5.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 6, Avg Loss: 0.0048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings at epoch 6: 100%|| 311/311 [00:04<00:00, 73.54it/s]\n",
      "Extracting negative pairs at epoch 6:  10%|                                      | 32/311 [00:01<00:11, 23.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved negative pair analysis data to ./embeddings_basic\\basic_pretrain_negative_pairs_epoch_6_20250331_193728.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 7/10: 100%|| 311/311 [00:58<00:00,  5.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 7, Avg Loss: 0.0037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 8/10: 100%|| 311/311 [00:55<00:00,  5.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 8, Avg Loss: 0.0038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings at epoch 8: 100%|| 311/311 [00:05<00:00, 59.65it/s]\n",
      "Extracting negative pairs at epoch 8:  10%|                                      | 32/311 [00:01<00:13, 21.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved negative pair analysis data to ./embeddings_basic\\basic_pretrain_negative_pairs_epoch_8_20250331_193728.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 9/10: 100%|| 311/311 [00:55<00:00,  5.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 9, Avg Loss: 0.0043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 10/10: 100%|| 311/311 [00:58<00:00,  5.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 10, Avg Loss: 0.0037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings at epoch 10: 100%|| 311/311 [00:03<00:00, 91.18it/s]\n",
      "Extracting negative pairs at epoch 10:  10%|                                     | 32/311 [00:01<00:13, 20.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved negative pair analysis data to ./embeddings_basic\\basic_pretrain_negative_pairs_epoch_10_20250331_193728.pkl\n",
      "\n",
      "Phase 2: Training GAN-CL...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 1/50: 100%|| 311/311 [02:08<00:00,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Losses: {'contrastive': 0.50136455562912, 'adversarial': -0.0005416347782039218, 'similarity': 0.000663293978136476, 'total': 0.5014308865909791}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting negative pairs at epoch 1:  10%|                                      | 32/311 [00:01<00:13, 20.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved negative pair analysis data to ./embeddings_basic\\basic_negative_pairs_epoch_1_20250331_193728.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 2/50: 100%|| 311/311 [02:38<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Losses: {'contrastive': 0.25942075977227697, 'adversarial': -0.00043530573497954574, 'similarity': 0.00060524426258792, 'total': 0.2594812843721013}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 3/50: 100%|| 311/311 [03:27<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Losses: {'contrastive': 0.20289996110645522, 'adversarial': -0.00040010992381265056, 'similarity': 0.000538927968029963, 'total': 0.20295385398282115}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 4/50: 100%|| 311/311 [04:50<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Losses: {'contrastive': 0.17774097727749413, 'adversarial': -0.00042217022340132335, 'similarity': 0.0004930709987198275, 'total': 0.1777902841448209}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 5/50: 100%|| 311/311 [06:21<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Losses: {'contrastive': 0.1533789767695058, 'adversarial': -0.0004736281641484972, 'similarity': 0.00041809148256969416, 'total': 0.15342078557974656}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 6/50: 100%|| 311/311 [06:09<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Losses: {'contrastive': 0.1467770218394002, 'adversarial': -0.0004500480473731791, 'similarity': 0.00038967181550761106, 'total': 0.1468159892683148}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 7/50: 100%|| 311/311 [04:54<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Losses: {'contrastive': 0.13663214397200435, 'adversarial': -0.0003915921107594858, 'similarity': 0.00047624430007531856, 'total': 0.13667976816103963}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 8/50: 100%|| 311/311 [02:00<00:00,  2.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Losses: {'contrastive': 0.1202896654282448, 'adversarial': -0.00048561302186633754, 'similarity': 0.00039455533522507626, 'total': 0.12032912090013456}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 9/50: 100%|| 311/311 [02:00<00:00,  2.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Losses: {'contrastive': 0.1191190708536427, 'adversarial': -0.0005540121493634987, 'similarity': 0.0004484434898922495, 'total': 0.11916391527537748}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 10/50: 100%|| 311/311 [01:59<00:00,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Losses: {'contrastive': 0.10979938667291518, 'adversarial': -0.00044098275154379836, 'similarity': 0.0004237935463957776, 'total': 0.10984176592272482}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting negative pairs at epoch 10:  10%|                                     | 32/311 [00:01<00:12, 21.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved negative pair analysis data to ./embeddings_basic\\basic_negative_pairs_epoch_10_20250331_193728.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 11/50: 100%|| 311/311 [01:58<00:00,  2.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Losses: {'contrastive': 0.11028230072289609, 'adversarial': -0.0004538125303776791, 'similarity': 0.0004510400029591066, 'total': 0.11032740489079639}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 12/50: 100%|| 311/311 [02:05<00:00,  2.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Losses: {'contrastive': 0.10199574703033235, 'adversarial': -0.0004591678568293787, 'similarity': 0.00046938733103044235, 'total': 0.10204268567311994}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 13/50: 100%|| 311/311 [02:04<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Losses: {'contrastive': 0.10504884395892118, 'adversarial': -0.0006323949316137701, 'similarity': 0.0004910717186199087, 'total': 0.1050979508733874}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 14/50: 100%|| 311/311 [02:05<00:00,  2.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Losses: {'contrastive': 0.09433958357217995, 'adversarial': -0.0005321465664725883, 'similarity': 0.0005150759981763983, 'total': 0.09439109117870736}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 15/50: 100%|| 311/311 [02:04<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Losses: {'contrastive': 0.09993494952762337, 'adversarial': -0.0005894923459426305, 'similarity': 0.0005592710975448262, 'total': 0.09999087682574796}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 16/50: 100%|| 311/311 [01:59<00:00,  2.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Losses: {'contrastive': 0.08902494018220633, 'adversarial': -0.0006586608500893668, 'similarity': 0.0005101271621640352, 'total': 0.08907595303784038}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 17/50: 100%|| 311/311 [02:15<00:00,  2.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Losses: {'contrastive': 0.0905823040951247, 'adversarial': -0.0005370177870610328, 'similarity': 0.0006223581413142503, 'total': 0.09064453965130823}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 18/50: 100%|| 311/311 [02:00<00:00,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Losses: {'contrastive': 0.08499311810131768, 'adversarial': -0.0006203898066809024, 'similarity': 0.0006077567102698987, 'total': 0.0850538937644464}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 19/50: 100%|| 311/311 [02:00<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Losses: {'contrastive': 0.0831616149790034, 'adversarial': -0.0006858021738846155, 'similarity': 0.0006052185335852373, 'total': 0.08322213673694628}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 20/50: 100%|| 311/311 [01:59<00:00,  2.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Losses: {'contrastive': 0.08349509726778583, 'adversarial': -0.0006193699250806279, 'similarity': 0.0006136753677968475, 'total': 0.08355646471154556}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting negative pairs at epoch 20:  10%|                                     | 32/311 [00:01<00:12, 22.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved negative pair analysis data to ./embeddings_basic\\basic_negative_pairs_epoch_20_20250331_193728.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 21/50: 100%|| 311/311 [07:27<00:00,  1.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, Losses: {'contrastive': 0.08397484708493545, 'adversarial': -0.0006634392794311478, 'similarity': 0.0006474053536405505, 'total': 0.08403958765927619}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 22/50: 100%|| 311/311 [02:21<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, Losses: {'contrastive': 0.08484803524670877, 'adversarial': -0.0006886168523059733, 'similarity': 0.0008267861749966392, 'total': 0.08493071373917425}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 23/50: 100%|| 311/311 [02:16<00:00,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23, Losses: {'contrastive': 0.08129937231983882, 'adversarial': -0.0006953834271929871, 'similarity': 0.0006993419988233584, 'total': 0.08136930667055572}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 24/50: 100%|| 311/311 [02:27<00:00,  2.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24, Losses: {'contrastive': 0.08447937009155367, 'adversarial': -0.0007784827597367419, 'similarity': 0.0009667406567421793, 'total': 0.08457604404598762}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 25/50: 100%|| 311/311 [02:24<00:00,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, Losses: {'contrastive': 0.07890967362024297, 'adversarial': -0.0007993545811069332, 'similarity': 0.000905983546416356, 'total': 0.07900027177293584}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 26/50: 100%|| 311/311 [02:02<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26, Losses: {'contrastive': 0.07584921540422934, 'adversarial': -0.0008081628934823133, 'similarity': 0.0008510500082216102, 'total': 0.0759343203199111}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 27/50: 100%|| 311/311 [02:15<00:00,  2.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27, Losses: {'contrastive': 0.07156761536087206, 'adversarial': -0.0008462849640215589, 'similarity': 0.0008466726646629848, 'total': 0.07165228273012822}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 28/50: 100%|| 311/311 [02:29<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28, Losses: {'contrastive': 0.07173194750017484, 'adversarial': -0.0009517971489135284, 'similarity': 0.0008772760436876147, 'total': 0.07181967527476825}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 29/50: 100%|| 311/311 [02:33<00:00,  2.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29, Losses: {'contrastive': 0.07480510711442331, 'adversarial': -0.0008915184502537778, 'similarity': 0.0009173970906081882, 'total': 0.07489684697875447}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 30/50: 100%|| 311/311 [02:36<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30, Losses: {'contrastive': 0.07581887496455642, 'adversarial': -0.00094641131065169, 'similarity': 0.0010971713444694181, 'total': 0.07592859203175573}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting negative pairs at epoch 30:  10%|                                     | 32/311 [00:02<00:20, 13.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved negative pair analysis data to ./embeddings_basic\\basic_negative_pairs_epoch_30_20250331_193728.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 31/50: 100%|| 311/311 [03:32<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31, Losses: {'contrastive': 0.066221291423922, 'adversarial': -0.0009359239917465596, 'similarity': 0.0010735352538223603, 'total': 0.06632864503549274}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 32/50: 100%|| 311/311 [09:17<00:00,  1.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32, Losses: {'contrastive': 0.07044738820990566, 'adversarial': -0.000997564901741539, 'similarity': 0.0010034095123859416, 'total': 0.07054772926344749}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 33/50: 100%|| 311/311 [05:45<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33, Losses: {'contrastive': 0.061763629011833784, 'adversarial': -0.0009628932603404309, 'similarity': 0.0009671930785940726, 'total': 0.06186034815787215}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 34/50: 100%|| 311/311 [06:18<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34, Losses: {'contrastive': 0.06971270815765791, 'adversarial': -0.0009933982856371035, 'similarity': 0.0010317949575545727, 'total': 0.06981588756198429}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 35/50: 100%|| 311/311 [06:08<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35, Losses: {'contrastive': 0.062114514638085844, 'adversarial': -0.001045116752691543, 'similarity': 0.0011780796317275842, 'total': 0.06223232258063325}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 36/50: 100%|| 311/311 [02:40<00:00,  1.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36, Losses: {'contrastive': 0.05977158792993695, 'adversarial': -0.0012233677599803692, 'similarity': 0.0011431035826018625, 'total': 0.05988589823964708}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 37/50: 100%|| 311/311 [03:29<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37, Losses: {'contrastive': 0.06739251827671405, 'adversarial': -0.0010998739628495582, 'similarity': 0.001118038550028248, 'total': 0.06750432206048268}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 38/50: 100%|| 311/311 [08:43<00:00,  1.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38, Losses: {'contrastive': 0.06339569971672113, 'adversarial': -0.001258933255254288, 'similarity': 0.0012536124079982327, 'total': 0.06352106081860864}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 39/50: 100%|| 311/311 [06:46<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39, Losses: {'contrastive': 0.06275455080499795, 'adversarial': -0.001188735825374986, 'similarity': 0.0011534777930728086, 'total': 0.06286989859607778}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 40/50: 100%|| 311/311 [03:23<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40, Losses: {'contrastive': 0.06594958598650633, 'adversarial': -0.0013097418349391969, 'similarity': 0.0011938829392335495, 'total': 0.06606897418881344}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting negative pairs at epoch 40:  10%|                                     | 32/311 [00:02<00:20, 13.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved negative pair analysis data to ./embeddings_basic\\basic_negative_pairs_epoch_40_20250331_193728.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 41/50: 100%|| 311/311 [02:42<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41, Losses: {'contrastive': 0.05724945919246823, 'adversarial': -0.0014291496176670913, 'similarity': 0.0014275409035321767, 'total': 0.05739221342470964}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 42/50: 100%|| 311/311 [02:41<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42, Losses: {'contrastive': 0.05833813713465209, 'adversarial': -0.001290801471590373, 'similarity': 0.0014077332810352038, 'total': 0.058478910607357666}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 43/50: 100%|| 311/311 [02:41<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43, Losses: {'contrastive': 0.05900636949603389, 'adversarial': -0.0013915332209414703, 'similarity': 0.001269931662969974, 'total': 0.05913336274971724}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 44/50: 100%|| 311/311 [02:40<00:00,  1.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44, Losses: {'contrastive': 0.06413406069417427, 'adversarial': -0.0012814767854857196, 'similarity': 0.0013533237942725037, 'total': 0.06426939318088329}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 45/50: 100%|| 311/311 [02:40<00:00,  1.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45, Losses: {'contrastive': 0.06049463499211661, 'adversarial': -0.0013912815184757665, 'similarity': 0.0013819150626659393, 'total': 0.0606328264953886}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 46/50: 100%|| 311/311 [02:42<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46, Losses: {'contrastive': 0.05883994511361578, 'adversarial': -0.0015525726468221454, 'similarity': 0.0014040900414534082, 'total': 0.05898035420524106}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 47/50: 100%|| 311/311 [02:46<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47, Losses: {'contrastive': 0.05377582113417255, 'adversarial': -0.0014578174499097097, 'similarity': 0.0014713240304132294, 'total': 0.053922953498864196}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 48/50: 100%|| 311/311 [03:53<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48, Losses: {'contrastive': 0.05343716798224441, 'adversarial': -0.0015239622258623145, 'similarity': 0.0014846446088042074, 'total': 0.053585632558959956}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 49/50: 100%|| 311/311 [05:23<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49, Losses: {'contrastive': 0.05132159933453827, 'adversarial': -0.0016706891199184575, 'similarity': 0.001514890779017911, 'total': 0.05147308844569891}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 50/50: 100%|| 311/311 [05:27<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50, Losses: {'contrastive': 0.05936186783273264, 'adversarial': -0.0016975478086488807, 'similarity': 0.001543041106786615, 'total': 0.05951617181546675}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting negative pairs at epoch 50:  10%|                                     | 32/311 [00:01<00:17, 16.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved negative pair analysis data to ./embeddings_basic\\basic_negative_pairs_epoch_50_20250331_193728.pkl\n",
      "Extracting post-training embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Post-training embeddings: 100%|| 311/311 [00:05<00:00, 59.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved embeddings and molecule data with graph properties to ./embeddings_basic\\final_embeddings_molecules_20250331_193728.pkl\n",
      "6. Training completed!\n",
      "7. Extracting final embeddings for analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings: 100%|| 311/311 [00:08<00:00, 35.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved embeddings and molecule data with graph properties to ./embeddings_basic/final_embeddings_basic_20250331_224412.pkl\n",
      "8. Final embeddings saved to ./embeddings_basic/final_embeddings_basic_20250331_224412.pkl\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Main function to run GAN-CL without momentum encoder training\"\"\"\n",
    "    # Enable anomaly detection during development\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    \n",
    "    # Set random seeds\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    random.seed(42)\n",
    "    \n",
    "    print(\"Starting data loading...\")    \n",
    "    extractor = MolecularFeatureExtractor()\n",
    "#     smiles_file = \"D:\\\\PhD\\\\Chapter3\\\\Unsupervised_GAN_Code\\\\pubchem-41-clean.txt\"\n",
    "    smiles_file = \"D:\\\\PhD\\\\Chapter3\\\\Unsupervised_GAN_Code\\\\pubchem-10m-clean_test10k.txt\"\n",
    "#     smiles_file = \"D:\\\\PhD\\\\Chapter3\\\\Unsupervised_GAN_Code\\\\pubchem-10m-clean_test50k.txt\"\n",
    "    \n",
    "    dataset = []\n",
    "    failed_smiles = []\n",
    "    smiles_list = []\n",
    "    \n",
    "    with open(smiles_file, 'r') as f:\n",
    "        for line in f:\n",
    "            smiles = line.strip()\n",
    "            smiles_list.append(smiles)\n",
    "            data = extractor.process_molecule(smiles)\n",
    "            if data is not None:\n",
    "                # Store SMILES as an attribute for later use\n",
    "                data.smiles = smiles\n",
    "                dataset.append(data)\n",
    "            else:\n",
    "                failed_smiles.append(smiles)\n",
    "    \n",
    "    print(f\"1. Loaded dataset with {len(dataset)} graphs.\")\n",
    "    print(f\"2. Failed SMILES count: {len(failed_smiles)}\")\n",
    "    \n",
    "    if not dataset:\n",
    "        print(\"No valid graphs generated.\")\n",
    "        return None\n",
    "        \n",
    "    # Setup training\n",
    "    batch_size = 32\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    print(f\"3. Created DataLoader with {len(train_loader.dataset)} graphs\")\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"4. Using device: {device}\")\n",
    "    \n",
    "    # Get configuration based on dataset\n",
    "    config = get_model_config(dataset)   \n",
    "   \n",
    "    # Train model\n",
    "    print(\"5. Starting basic GAN-CL training (without momentum encoder)...\")\n",
    "    model, metrics = train_gan_cl_basic(\n",
    "        train_loader, \n",
    "        config,\n",
    "        dataset,\n",
    "        device=device,\n",
    "        save_dir='./checkpoints_basic',\n",
    "        embedding_dir='./embeddings_basic'\n",
    "    )    \n",
    "    \n",
    "    print(\"6. Training completed!\")\n",
    "    \n",
    "    # Extract embeddings for analysis\n",
    "    print(\"7. Extracting final embeddings for analysis...\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        all_embeddings = []\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=\"Extracting embeddings\"):\n",
    "            batch = batch.to(device)\n",
    "            embeddings = model.get_embeddings(batch)\n",
    "            all_embeddings.append(embeddings.cpu())\n",
    "            \n",
    "    all_embeddings = torch.cat(all_embeddings, dim=0).numpy()\n",
    "    \n",
    "    # Create a timestamp for file naming\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    # Save final embeddings and graphs\n",
    "    final_embedding_path = f'./embeddings_basic/final_embeddings_basic_{timestamp}.pkl'\n",
    "    save_embeddings_with_molecules(all_embeddings, dataset, final_embedding_path)\n",
    "    \n",
    "    print(f\"8. Final embeddings saved to {final_embedding_path}\")\n",
    "    \n",
    "    return model, metrics, all_embeddings\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model, metrics, embeddings = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d928ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
